{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOdJoVQYdzCi"
      },
      "source": [
        "#Model Hub Login config (ToDO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrfxv4qBd4kb"
      },
      "source": [
        "Import /install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPJ_SnahHwmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf3fde5-0955-4cff-8eae-fb14a4a768fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.1)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.13.3)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install tokenizers\n",
        "!pip install transformers[torch]\n",
        "!pip install tqdm\n",
        "!pip install pynvml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki0EGyQd5mj8"
      },
      "source": [
        "# Data sets\n",
        "Load data from CSV files for test and dev as well as surpisal sorted csv file for train\n",
        "\n",
        "custom dataset class? https://www.youtube.com/watch?v=PXOzkkB5eH0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7OMmMZgd7Wlu"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, DatasetDict\n",
        "def create_ds_dict(ds_dir):\n",
        "  '''args:str ds_dir: fpath to directory with sentence CSV files\n",
        "          str train_type: rand or curriculum\n",
        "          lst int seeds: list of seeds for shuffling dataset splits\n",
        "     return DatasetDict ds_dict where {k-->ds_split:v-->ds with sentIDs and sentences}'''\n",
        "  datFiles={\"train\":(ds_dir+\"curriculum_lstmrev_sumsurp_root10.0_T150000.csv\"),\"validation\":(ds_dir+\"babylm_sent_dev.csv\"),\"test\":(ds_dir+\"babylm_sent_test.csv\")}\n",
        "  csv_dict=load_dataset(\"csv\", data_files=datFiles, sep=\"\\037\") #load train test dev split into a Datasetdictionary\n",
        "  ds_dict=DatasetDict()\n",
        "  for k,v in csv_dict.column_names.items(): #remove unecessary columns\n",
        "    ds_dict=csv_dict.remove_columns([col for col in v if col!='sentid'and col!='sentence'])\n",
        "  #shuffle data , according to train_type\n",
        "  '''if train_type==\"rand\":\n",
        "    print(\"pre-shuffle train:\",ds_dict[\"train\"][\"sentence\"][1])\n",
        "    ds_dict[\"train\"].shuffle(seed)\n",
        "    print(\"post-shuffle train\",ds_dict[\"train\"][\"sentence\"][1])\n",
        "  print(\"pre-shuffle valid:\",ds_dict[\"validation\"][\"sentence\"][1])\n",
        "  ds_dict[\"validation\"].shuffle(seed)\n",
        "  print(\"post-shuffle valid:\",ds_dict[\"validation\"][\"sentence\"][1])\n",
        "  print(\"pre-shuffle test:\",ds_dict[\"test\"][\"sentence\"][1])\n",
        "  ds_dict[\"test\"].shuffle(seed)\n",
        "  print(\"post-shuffle test:\",ds_dict[\"test\"][\"sentence\"][1])'''\n",
        "  return ds_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHPAlTqG5tCp"
      },
      "source": [
        "#Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWt2FwxUJZdW"
      },
      "source": [
        "###Loading pre-trained BPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLo9l5MLJdNA"
      },
      "source": [
        "### Tokenizer components, hugging face links for component options and , GPT 2 BPE specification\n",
        "1. [Normalization](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers) ( any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.):\n",
        "  *   GPT2 BPE does not use a normalizer\n",
        "2. [Pre-tokenization](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers) (splitting the input into words):\n",
        "  *   GPT2 BPE is not dependent on significant  pre-tokenization\n",
        "3. Running the input through the [model](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models) (using the pre-tokenized words to produce a sequence of tokens) and train tokenizer using model specific [trainer](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers):\n",
        "  *   GPT2 BPE uses the BPE model\n",
        "4. [Post-processing](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors) (adding the special tokens of the tokenizer, generating the attention mask and token type IDs):\n",
        "  *   GPT2 BPE uses????\n",
        "5. [Decoding](https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders) (technically not a compondent of tokenization , but the process through which one decodes the output of tokenization):\n",
        "  *   Logically, given a byte level encoder , should use a byte level decoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6cnJyPrJo6h"
      },
      "outputs": [],
      "source": [
        "#tokenizer related libraries\n",
        "import tokenizers\n",
        "from tokenizers import (\n",
        "    normalizers, #1\n",
        "    Tokenizer,\n",
        "    pre_tokenizers, #2\n",
        "    models, #3\n",
        "    trainers,#3\n",
        "    processors, #4\n",
        "    decoders, #5\n",
        ")\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "def create_BPE_tok(sents,save_dir):\n",
        "  '''args: str list sents\n",
        "           str save_dir\n",
        "     returns: PreTrainedTokenizerFast wrapped tokenizer'''\n",
        "  tokenizer = Tokenizer(models.BPE()) #tokenizer objects are intialized with a model argument which specifies with algorithm they will use for tokenization\n",
        "  #GPT-2 does not use a normalizer, so we skip #1 and go directly to #2: pre-tokenization\n",
        "  tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False) #prevents default addition of space char at the beginning of a sentence\n",
        "  #3 passing input to model and training tokenizer\n",
        "  trainer = trainers.BpeTrainer(vocab_size=50272, special_tokens=[\"<|endoftext|>\", '<image>', '</c>', '<PERSON>']) # EOS is apparently the only special token for BPE (rest based on baseline github) , set vocab size according to babyLM baseline model config val\n",
        "  tokenizer.train_from_iterator(sents, trainer=trainer)#tokenizer.train([\"train.txt\"], trainer=trainer) #can use train from iterator to file by file\n",
        "  tokenizer.post_processor = processors.ByteLevel(trim_offsets=False) #4\n",
        "  tokenizer.decoder = decoders.ByteLevel()#5\n",
        "\n",
        "  ###Save (and Push?) Tokenizer\n",
        "  wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "      tokenizer_object=tokenizer,\n",
        "      bos_token=\"<|endoftext|>\",\n",
        "      eos_token=\"<|endoftext|>\",\n",
        "  )\n",
        "  #usable by HF to recreate this tokenizer\n",
        "  wrapped_tokenizer.save_pretrained(save_dir+\"babyLM10M-byte-level-BPE-tokenizer\")\n",
        "  #tokenizer.push_to_hub(\"babyLM10M-byte-level-BPE-tokenizer\") #usable if logged into HF hub\n",
        "  return wrapped_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0D1lGMc7-5Sb"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "#Code for loading tokenizer from HF wrapper (more likely to work)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"babyLM10M-byte-level-BPE-tokenizer\") #Could specify path to a checkpoint or a tokenizer directory (requires that tokenizer was saved using HF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt52DnknQbOr"
      },
      "source": [
        "# Get Batches (data loading)\n",
        "1.   Order sentences:Shuffled during dataset laoding (depending on whether currOPT or randOPT)\n",
        "2.   Tokenize and truncate sentences according to context length\n",
        "3.   use data collator to take tokenized list and dynamically pad (pad to longest) as per batch size (32 sequences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sYqGLqxr60Io"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import csv\n",
        "def dynamically_pad_pretokenized(tokenized_batch,tokenizer):\n",
        "  '''arg: batch of inputIds and corresponding tokenizer\n",
        "     return: dynamically padded batch of pt tensors for causal LM'''\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False , return_tensors=\"pt\")\n",
        "  tensor=data_collator(tokenized_batch)\n",
        "  return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uomYF4y48PrW"
      },
      "outputs": [],
      "source": [
        "def collate_and_log_batch(tokenized_batch,tokenizer,batches_so_far,log_file):\n",
        "  '''arg: batch of inputIds and corresponding tokenizer , log_file\n",
        "     log: info about tensor dimensions into a csv\n",
        "     return: dynamically padded batch of pt tensors for causal LM'''\n",
        "  #print(len(tokenized_batch))\n",
        "  collated_batch=dynamically_pad_pretokenized(tokenized_batch,tokenizer)\n",
        "  #print(len(collated_batch))\n",
        "  fields = collated_batch.keys()\n",
        "\n",
        "  ###Logging#####\n",
        "  if batches_so_far==0:# initialize tensor dim log csv with 1st tensor\n",
        "      with open(log_file, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Key', 'Size 1', 'Size 2'])\n",
        "        for k, v in collated_batch.items():\n",
        "          writer.writerow([k, v.size(0), v.size(1)])\n",
        "  else:\n",
        "      with open(log_file, 'a', newline='') as file:  # Open the file in append mode\n",
        "            writer = csv.writer(file)\n",
        "            for k, v in collated_batch.items():\n",
        "              writer.writerow([k, v.size(0), v.size(1)])\n",
        "  return collated_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xTTNMy7733ns"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "def create_tokenized_batches(sents,tokenizer,context_length,batch_size,log_file):\n",
        "  '''Args: str list sents: ordered list of sentences to iterate over\n",
        "           tokenizer\n",
        "           context_length: num of previous tokens that can be used to predict next token in pretraining\n",
        "           batch_size: num sequences of len<=context_length that can be processed by model in a step\n",
        "           log_file: CSV file to log tensor dimensions to\n",
        "      return:array of pt tensors containing dynamically padded batches'''\n",
        "  collated_batches=[]\n",
        "  batch=[]\n",
        "  for count,sent in tqdm(enumerate(sents, start=1),total=len(sents)):\n",
        "    outputs = tokenizer(\n",
        "        sent,\n",
        "        truncation=True, #chunk sentence according to context length if too long\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    for input_id in outputs[\"input_ids\"]:#input chunks count as seperate sequences\n",
        "      if len(batch)<batch_size:\n",
        "        batch.append(input_id)\n",
        "      else: #batch_size reached ,  collate into tensor\n",
        "        collated_batch=collate_and_log_batch(batch,tokenizer,len(collated_batches),log_file)\n",
        "        collated_batches.append(collated_batch)\n",
        "        #print(len(batch))\n",
        "        batch=[]\n",
        "        batch.append(input_id)\n",
        "  #add last batch (potentially smaller than batch size)\n",
        "  collated_batch=collate_and_log_batch(batch,tokenizer,len(collated_batches),log_file)\n",
        "  collated_batches.append(collated_batch)\n",
        "  print(\"batches created:\",len(collated_batches))\n",
        "  return collated_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw51StwP5vA0"
      },
      "source": [
        "#OPT 125M model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NafHz4P-_lYM"
      },
      "source": [
        "##Configuration and initialization\n",
        "Intialize an OPT model to closely replicat OPT125M architecture , using details from [(Zhang et al., 2022) ](https://https://arxiv.org/pdf/2205.01068.pdf) and [babyLM OPT125M vaseline](https://huggingface.co/babylm/opt-125m-strict-small/blob/main/config.json). See further details on the 15 params at huggingface OPTconfig documentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N4MKyJib_nZz"
      },
      "outputs": [],
      "source": [
        "from transformers.models.opt import OPTConfig\n",
        "from transformers.models.opt.modeling_opt import OPTForCausalLM\n",
        "def init_OPT_CLM():\n",
        "  '''initialize a OPT 125M model with a 0.2M param causal language modelling head for pre-training , according configuration params rationalized in comments'''\n",
        "  #Set config params\n",
        "  config=OPTConfig()\n",
        "  print(config) #check defaults\n",
        "  config.vocab_size=50272 #vocab size used in babylm baseline\n",
        "  config.num_hidden_layers=768 #See dLayers , table 1 zhang et Al.\n",
        "  config.num_hidden_layers=12 #See #L , table 1 zhang et Al.\n",
        "  config.ffn_dim=3072 #see OPT125m baseline\n",
        "  config.num_attention_heads=12 #See #H , table 1 zhang et Al.\n",
        "  config.activation_function=\"relu\"#see zhang et al\n",
        "  config.max_position_embeddings=2048 #see OPT125m baseline\n",
        "  config.do_layer_norm_before=True #see OPT125m baseline\n",
        "  config.word_embed_proj_dim=768 #generally equal to hidden size , intialized accoring to OPT125M baseline\n",
        "  config.dropout=0.1 # see Zhang et Al : \"We use a dropout of 0.1 throughout, but we do not apply any dropout to embeddings.\"\n",
        "  config.attention_dropout=0.0 #see OPT125M baseline\n",
        "  config.init_std=0.02 #Intialized according to OPT125M baseline however potnetially signifcant differences from zhang et Al: “For weight initialization ….. using a normal distribution with zero mean and standard deviation of 0.006.”” Standard deviation for output layers are scaled by a 1.0/ √ 2L term where L is the total number of layers. ”\n",
        "  config.use_cache=True #see OPT125M baseline\n",
        "  config.enable_bias=True #see OPT125M baseline\n",
        "  config.layer_norm_elementwise_affine=True #see OPT125M baseline\n",
        "  print(config) #check updates\n",
        "  #intialize OPTm model with a causalLM head for training\n",
        "  OPTCLM=OPTForCausalLM(config)\n",
        "  #check architecture\n",
        "  print(OPTCLM)\n",
        "  model_size = sum(t.numel() for t in OPTCLM.parameters())\n",
        "  print(f\"OPT size: {model_size/1000**2:.1f}M parameters\")\n",
        "  return OPTCLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mHEdDwq52Fc"
      },
      "source": [
        "## Optimizer and Scheduler\n",
        "\n",
        "OPT paper:\"We follow a linear learning\n",
        "rate schedule, warming up from 0 to the maximum\n",
        "learning rate over the first 2000 steps in OPT-175B,\n",
        "or over 375M tokens in our smaller baselines, and\n",
        "decaying down to 10% of the maximum LR over\n",
        "300B tokens. A number of mid-flight changes\n",
        "to LR were also required (see Section 2.5). Our\n",
        "batch sizes range from 0.5M to 4M depending on\n",
        "the model size (see Table 1) and is kept constant\n",
        "throughout the course of training.\"\n",
        "\n",
        "Platanios et al:\n",
        "*   for large experiments with tranformers T=50K (see curriculum hyperparameters) (large dataset was WMT with 4.5M sentences)\n",
        "*   Twarmup  \"is set to 10,000 in these experiments\" => 1/5 of steps\n",
        "*    \"This schedule was proposed in the original\n",
        "Transformer paper (Vaswani et al., 2017)\"\n",
        "\n",
        "Vaswani et al., 2017:\n",
        "* \"We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
        "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
        "(3.5 days).\"\n",
        "* Also used linear learning rate \"We used warmup_steps = 4000.\" => small fraction\n",
        "*Also had the same large WMT 4.5M sentence dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oZ5t-TySA7Zp"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "from transformers import AdamW\n",
        "def create_optimizer_and_lr_scheduler(total_training_steps):\n",
        "  '''args: int total_training_steps: int epochs*int batches per epoch\n",
        "     returns AdamW optimizer , linear learning rate scheduler (configured as per  OPT paper) '''\n",
        "  optimizer = AdamW(model.parameters(), lr=5e-5,betas=(0.9,0.95),weight_decay=0.1) #Zhang et Al \"We use an AdamW optimizer (Loshchilov andHutter, 2017) with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1\"\n",
        "  lr_scheduler = get_scheduler(\n",
        "      name=\"linear\", #zhang et Al. followed a linear learning rate schedule , midflight changes not relevant to 125M model and  our small dataset\n",
        "      optimizer=optimizer,\n",
        "      num_warmup_steps=0, #int(total_training_steps*0.05), #TODO set as a fraction of training steps=> going with 5% #NO WARMUP FOR CURRICULUM\n",
        "      num_training_steps=total_training_steps\n",
        "   )\n",
        "  return optimizer, lr_scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuF_pA8s6Btc"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt#the-evaluation-loop\n",
        "\"Since we want to evaluate the model regularly on the validation set during training, let’s write a function for that as well. It just runs through the evaluation dataloader and gathers all the losses across processes:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z6qY4-32Hmf_"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "def evaluate(model,eval_dataloader,device):\n",
        "  '''evaluate model to get loss and perplexity'''\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  for step, batch in tqdm(enumerate(eval_dataloader,start=1),total=len(eval_dataloader)):\n",
        "     batch_on_device = {k: v.to(device) for k, v in batch.items()}\n",
        "     with torch.no_grad():\n",
        "          outputs = model(batch_on_device[\"input_ids\"], labels=batch_on_device[\"input_ids\"])\n",
        "     losses.append(outputs.loss)#losses.append(accelerator.gather(outputs.loss))### change\n",
        "     '''if step %100==0:\n",
        "            print({\"step\":step,\"loss\":(outputs.loss).item(),\"loss size\":(outputs.loss).size()})'''\n",
        "  loss = torch.mean(torch.stack(losses))#torch.mean(torch.cat(losses)) cant concat 0 dim tensors\n",
        "  try:\n",
        "      perplexity = torch.exp(loss)\n",
        "  except OverflowError:\n",
        "      perplexity = float(\"inf\")\n",
        "  print({\"loss\":loss,\"ppl\":perplexity.item()})\n",
        "  return loss, perplexity.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNZ1hxam-LOg"
      },
      "source": [
        "##Early Stopping\n",
        "class adapted with minimal changes from : https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mKz4YeGe-QaO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(32)\n",
        "class EarlyStopping(object):\n",
        "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
        "        '''params:str mode: Indicates whether to minimize ('min') or maximize ('max') the monitored metric.\n",
        "                 float min_delta: The minimum change required to qualify as an improvement in the monitored metric.\n",
        "                 int patience: The number of epochs to wait for an improvement before stopping the training process.\n",
        "                 bool percentage: If True, min_delta is interpreted as a percentage change rather than an absolute value.'''\n",
        "        self.mode = mode\n",
        "        self.min_delta = min_delta\n",
        "        self.patience = patience\n",
        "        self.best = None\n",
        "        self.num_bad_epochs = 0\n",
        "        self.is_better = None\n",
        "        self._init_is_better(mode, min_delta, percentage)\n",
        "\n",
        "        # If patience is set to 0, always consider the current metric as the best\n",
        "        if patience == 0:\n",
        "            self.is_better = lambda a, b: True\n",
        "            self.step = lambda a: False\n",
        "\n",
        "    def step(self, metrics):\n",
        "        if self.best is None:\n",
        "            self.best = metrics\n",
        "            return False\n",
        "\n",
        "        # If the metric is NaN, consider it as a signal to stop early\n",
        "        if torch.isnan(metrics):#np.isnan(metrics):# consider using torch.isnan(metrics) :\n",
        "            return True\n",
        "\n",
        "        # Check if the current metric is better than the best\n",
        "        if self.is_better(metrics, self.best):\n",
        "            self.num_bad_epochs = 0\n",
        "            self.best = metrics\n",
        "        else:\n",
        "            self.num_bad_epochs += 1\n",
        "\n",
        "        # If the number of bad epochs exceeds patience, stop early\n",
        "        if self.num_bad_epochs >= self.patience:\n",
        "            print('Terminating because of early stopping!')\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _init_is_better(self, mode, min_delta, percentage):\n",
        "        if mode not in {'min', 'max'}:\n",
        "            raise ValueError('Mode ' + mode + ' is unknown!')\n",
        "        if not percentage:\n",
        "            if mode == 'min':\n",
        "                # Check if a is smaller than (best - min_delta)\n",
        "                self.is_better = lambda a, best: a < best - min_delta\n",
        "            if mode == 'max':\n",
        "                # Check if a is larger than (best + min_delta)\n",
        "                self.is_better = lambda a, best: a > best + min_delta\n",
        "        else:\n",
        "            if mode == 'min':\n",
        "                # Check if a is smaller than (best - (best * min_delta / 100))\n",
        "                self.is_better = lambda a, best: a < best - (best * min_delta / 100)\n",
        "            if mode == 'max':\n",
        "                # Check if a is larger than (best + (best * min_delta / 100))\n",
        "                self.is_better = lambda a, best: a > best + (best * min_delta / 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "VdFjT4h5BCVb",
        "outputId": "249070a3-9570-4d03-98da-ef84feadbc33"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_one_epoch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 7\u001b[0m       \u001b[43mtrain_one_epoch\u001b[49m(model, data_loader)  \u001b[38;5;66;03m# train the model for one epoch.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m       metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, data_loader_dev)  \u001b[38;5;66;03m# evalution on dev set.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m es\u001b[38;5;241m.\u001b[39mstep(metric):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_one_epoch' is not defined"
          ]
        }
      ],
      "source": [
        "#Example\n",
        "es = EarlyStopping(patience=5)\n",
        "\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "      train_one_epoch(model, data_loader)  # train the model for one epoch.\n",
        "      metric = eval(model, data_loader_dev)  # evalution on dev set.\n",
        "      if es.step(metric):\n",
        "          break  # early stop criterion is met, we can stop now\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5IVp-0dhWEj"
      },
      "source": [
        "# Logging\n",
        "##TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7MTa89OhiOj",
        "outputId": "b9da1c11-421d-4b14-e528-597d3c71374a"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unexpected indent (1643835284.py, line 6)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    writer.add_scalars('Training vs. Validation Loss',\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer=SummaryWriter(log_dir)\n",
        "\n",
        "####Log scalars\n",
        " # Log the running loss averaged per batch\n",
        "            writer.add_scalars('Training vs. Validation Loss',\n",
        "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                            epoch * len(training_loader) + i)\n",
        "####Visualize network\n",
        "# Again, grab a single mini-batch of images\n",
        "dataiter = iter(training_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# add_graph() will trace the sample input through your model,\n",
        "# and render it as a graph.\n",
        "writer.add_graph(net, images)\n",
        "writer.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYEW-W3ompxH"
      },
      "source": [
        "## CSV writer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mziLykrOmuSL"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def log_prog(dicts, fname, curr_step, log_steps):\n",
        "    '''Args:\n",
        "       - list[dict] dicts: Training loop logging entries for a list of batches.\n",
        "       - str fname: Path of the file to append entries to.\n",
        "       - int curr_step: The training step at which the logging is happening. (midrun:should be completed_step instead of step)\n",
        "       - int log_steps: This function is called every log_steps steps. curr_step%log_steps ==0\n",
        "       Returns: None'''\n",
        "\n",
        "    try: # avoid inerupting training loop\n",
        "        fields = dicts[0].keys()\n",
        "        if curr_step/log_steps == 1:  # Check if it's the 1st logging instance\n",
        "            with open(fname, 'w', newline='') as file:\n",
        "                writer = csv.DictWriter(file, fieldnames=fields)\n",
        "                writer.writeheader()\n",
        "                writer.writerows(dicts)\n",
        "        else:\n",
        "            with open(fname, 'a', newline='') as file:  # Open the file in append mode\n",
        "                writer = csv.writer(file)\n",
        "                for dictionary in dicts:\n",
        "                  writer.writerow(dictionary.values())\n",
        "    except Exception as e:\n",
        "        print(f\"Logging failed: {e}\")\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSLYqdpz57Ol"
      },
      "source": [
        "#Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3flbd682i00F"
      },
      "source": [
        "Refactoring giant training loop code into functions (In progress)\n",
        "\n",
        "See code used under experiments for training loops tested"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fiHKlfLfGVAc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_one_step(model,device,batch,optimizer,lr_scheduler):\n",
        "  '''Args: model to train one step (forward and backward pass)\n",
        "                 associated optimizer and scheduler\n",
        "            pt tensor batch to be passed through model\n",
        "     return: current loss and learning rate for logging '''\n",
        "  #Forward Pass\n",
        "  batch_on_device = {k: v.to(device) for k, v in batch.items()}\n",
        "  fpass_output=model(input_ids=batch_on_device[\"input_ids\"],labels=batch_on_device[\"labels\"]) # Forward pass: **batch ??\n",
        "  logits = fpass_output.logits  #Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax):logits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size))\n",
        "  loss =fpass_output.loss #cross-entropy loss: loss (torch.FloatTensor of shape (1,), optional, returned when labels is provided) — Language modeling loss (for next-token prediction).\n",
        "  #update logging vars\n",
        "  curr_loss=loss.item()\n",
        "  curr_lr=lr_scheduler.get_lr()\n",
        "  #backward pass\n",
        "  loss.backward(loss)\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip the gradients to prevent exploding gradients => used 1.0 to 0.3 clipping in OPT paper\n",
        "  optimizer.step()\n",
        "  lr_scheduler.step()\n",
        "  optimizer.zero_grad()  # Clear gradients from the previous optimization step\n",
        "  return curr_loss , curr_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-uwzkIaNFfLc"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "def train_one_epoch(model,lr_scheduler,optimizer,device,epoch,train_dataloader,eval_dataloader,total_training_steps,completed_steps,log_steps,t_log,eval_steps,e_log,early_stopper,spath):\n",
        "  '''Args:  model and associated lr_scheduler and optimizer being trained on device\n",
        "            int epoch being trained\n",
        "            tensor list train and eval dataloaders\n",
        "            int steps\n",
        "            fnames for log_files training and eval\n",
        "            EarlyStopping early_stopper\n",
        "      return  step, boolean converged '''\n",
        "  progress=[]\n",
        "  for step, batch in tqdm(enumerate(train_dataloader, start=1), total=total_training_steps): #iterate with progress bar where  'step'= current batch index, 'batch' contains the input data for the current batch , represent batch 0 as batch 1\n",
        "    loss,lr=train_one_step(model,device,batch,optimizer,lr_scheduler)\n",
        "    completed_steps += 1\n",
        "    prog={\"steps\": completed_steps, \"loss/train\": loss,\"lr\": lr,\"tensor_memory\":torch.cuda.memory_allocated(device)}\n",
        "    progress.append(prog)\n",
        "    if (epoch*step) % (epoch*log_steps) == 0: #LOGGING progress metrics computed on train set every log_steps\n",
        "      log_prog(progress,t_log,completed_steps, log_steps)\n",
        "      progress=[]\n",
        "    if ((epoch*step) % (epoch*eval_steps)) == 0:# Perform model evaluation on validation data after every eval_steps, set eval_steps=len(train_dataloader) for per epoch logging (generalizes to currOPT)\n",
        "            eval_loss, perplexity = evaluate(model,eval_dataloader,device)  # Evaluate the model's performance on the evaluation dataset\n",
        "            result=[{\"step\":completed_steps,\"loss/eval\": eval_loss.item(), \"perplexity\": perplexity}]\n",
        "            log_prog(result,e_log,completed_steps,eval_steps)\n",
        "            #####check pointing code######\n",
        "            torch.save({\n",
        "            'epoch': epoch,\n",
        "            'completed_steps':completed_steps,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'lr_scheduler': lr_scheduler.state_dict(),\n",
        "            'lr':lr,\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'eval_loss': eval_loss,\n",
        "            }, spath+str(completed_steps)+'.pt') #save to path +name+'step:'+completed_steps+'.pt'\n",
        "            '''if early_stopper.step(eval_loss):#not for currOPT\n",
        "              return completed_steps ,True #converged (changed from steps to completed_steps midrun)'''\n",
        "            model.train()  # Set the model back to training mode\n",
        "  log_prog(progress,t_log,completed_steps,log_steps) #log leftover values (changed midrun)\n",
        "  return completed_steps, False #will continue training unless last epoch (changed from step to completed steps midrun)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wnoNdLYbETYN"
      },
      "outputs": [],
      "source": [
        "def training_loop(model,lr_scheduler,optimizer,device,num_epochs,train_dataloader,eval_dataloader,total_training_steps,completed_steps,log_steps,t_log,eval_steps,e_log,early_stopper,output_dir):\n",
        "  tval=0\n",
        "  for epoch in range(1,num_epochs+1):\n",
        "    tval,converged=train_one_epoch(model,lr_scheduler,optimizer,device,epoch,train_dataloader,eval_dataloader,total_training_steps,completed_steps,log_steps,t_log,eval_steps,e_log,early_stopper,spath=output_dir)\n",
        "    if converged:\n",
        "       model.save_pretrained(output_dir)\n",
        "       return tval\n",
        "    #else prepare for next epoch\n",
        "    completed_steps=tval\n",
        "    random.shuffle(train_dataloader)\n",
        "    random.shuffle(eval_dataloader)\n",
        "  model.save_pretrained(output_dir)\n",
        "  return tval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFzgMQdHUbXU",
        "outputId": "3df93a27-a47e-4305-eb1a-f4a68b305678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "#Initial params\n",
        "random.seed(32) #set seed\n",
        "num_train_epochs=1\n",
        "context_length=128\n",
        "batch_size=32\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)\n",
        "sync_dir='/home/sharedDATA/models/'\n",
        "#directories\n",
        "tok_dir=\"\" #store tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Rs9NDhUhaZI"
      },
      "outputs": [],
      "source": [
        "import pynvml\n",
        "from pynvml import *  # provides access to NVIDIA's NVML (NVIDIA Management Library).enables  querying and monitoring of NVIDIA GPU devices.\n",
        "\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()  # Initializing the NVML library\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)  # Obtaining a handle for the GPU device at index 0\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)  # Retrieving memory information for the GPU device\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")  # Printing the GPU memory occupied in megabytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skZhdUX1haZJ",
        "outputId": "83f0ce2a-a963-4e5f-edad-222f766f1eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory occupied: 440 MB.\n"
          ]
        }
      ],
      "source": [
        "print_gpu_utilization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoOOaisR1Htv",
        "colab": {
          "referenced_widgets": [
            "ddbf0b5aad2f4bca8c39ce10a59c12f3"
          ]
        },
        "outputId": "e9d8d5de-814a-48a1-beaa-7f839d09d055"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset csv (/home/achobey/.cache/huggingface/datasets/csv/default-63f4202a69827022/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddbf0b5aad2f4bca8c39ce10a59c12f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train[1]: kee .\n",
            "train[1] post shuffle: And then Cannata and Doss order a hit... 'cause she snuffed out their old friend.\n",
            "val[1]: i yeah it's more yeah .\n",
            "val[1] post shuffle: Do you think he's crying, weeping violently in the depths of the sea?\n",
            "test[1] post shuffle: kangaroo .\n",
            "test[1] post shuffle: Yes, but I think they're not unrelated.\n"
          ]
        }
      ],
      "source": [
        "#load datasets\n",
        "ds_dict=create_ds_dict(\"\")\n",
        "curr_root10_train=ds_dict[\"train\"][\"sentence\"]\n",
        "print(\"curr_root10_train[1]:\",train[1])\n",
        "#random.shuffle(train) don't shuffle curr_train\n",
        "#print(\"train[1] post shuffle:\", train[1])\n",
        "val=ds_dict[\"validation\"][\"sentence\"]\n",
        "print(\"val[1]:\", val[1])\n",
        "random.shuffle(val)\n",
        "print(\"val[1] post shuffle:\", val[1])\n",
        "test=ds_dict[\"test\"][\"sentence\"]\n",
        "print(\"test[1] pre shuffle:\", test[1])\n",
        "random.shuffle(test)\n",
        "print(\"test[1] post shuffle:\", test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d500175d9a6d42f9877022a0ceb509fb"
          ]
        },
        "id": "ToI8gSXrhaZK",
        "outputId": "99a7fa28-bae5-4cbc-c433-78dcc8dea7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d500175d9a6d42f9877022a0ceb509fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/918982 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batches created: 28937\n",
            "OPTConfig {\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "OPTConfig {\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "OPTForCausalLM(\n",
            "  (model): OPTModel(\n",
            "    (decoder): OPTDecoder(\n",
            "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
            "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
            "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x OPTDecoderLayer(\n",
            "          (self_attn): OPTAttention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
            ")\n",
            "OPT size: 125.2M parameters\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
              "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#tokenizer=create_BPE_tok(train,tok_dir) #use BPE trained previously\n",
        "trainDL=create_tokenized_batches(curr_root10_train,tokenizer,context_length,batch_size,\"currtrainDL.csv\")\n",
        "batches_per_epoch=len(trainDL)\n",
        "total_steps=num_train_epochs*batches_per_epoch #update gradient every batch\n",
        "model=init_OPT_CLM() #OPT125M with causal LM head for training\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81zBhu0T1RxO",
        "colab": {
          "referenced_widgets": [
            "5cb394a2677a4558b4033336093ec5ff"
          ]
        },
        "outputId": "b09447c3-c3f7-441c-ce69-9a23bc168eb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdamW (\n",
            "Parameter Group 0\n",
            "    betas: (0.9, 0.95)\n",
            "    correct_bias: True\n",
            "    eps: 1e-06\n",
            "    initial_lr: 5e-05\n",
            "    lr: 0.0\n",
            "    weight_decay: 0.1\n",
            ")\n",
            "<torch.optim.lr_scheduler.LambdaLR object at 0x7f468a1db2b0>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/tljh/user/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cb394a2677a4558b4033336093ec5ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/883051 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batches created: 27806\n"
          ]
        }
      ],
      "source": [
        "es=EarlyStopping(patience=3)\n",
        "optimizer,lr_scheduler=create_optimizer_and_lr_scheduler(total_steps)\n",
        "print(optimizer)\n",
        "print(lr_scheduler)\n",
        "valDL=create_tokenized_batches(val,tokenizer,context_length,batch_size,\"valDL.csv\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HcrtecJ1WFB",
        "colab": {
          "referenced_widgets": [
            "0df71e85a4ef4b15ad43fd9b12881ba8",
            "d42669e973a9459eb3932236d3054a35",
            "3fcded06e3f94c62a886f5103621a100",
            "9b256fadfe584d1f8a11d828a4008b0a",
            "2cee352272aa4bf1849f77e44d995739",
            "4514d25ad8644b329661dd16a19d1db4",
            "a8a252a8ed4f4e7face0d1e58583a3a2"
          ]
        },
        "outputId": "6caa1506-1396-4f99-c4cb-75dc8c06a962"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0df71e85a4ef4b15ad43fd9b12881ba8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/578740 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': tensor(11.4133, device='cuda:0'), 'ppl': 90514.3203125}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d42669e973a9459eb3932236d3054a35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/578740 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': tensor(10.2444, device='cuda:0'), 'ppl': 28125.513671875}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fcded06e3f94c62a886f5103621a100",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/578740 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b256fadfe584d1f8a11d828a4008b0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/27806 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': tensor(9.8626, device='cuda:0'), 'ppl': 19198.693359375}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cee352272aa4bf1849f77e44d995739",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/578740 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4514d25ad8644b329661dd16a19d1db4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/27806 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub message rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': tensor(9.9055, device='cuda:0'), 'ppl': 20040.69921875}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8a252a8ed4f4e7face0d1e58583a3a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/578740 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': tensor(10.0648, device='cuda:0'), 'ppl': 23501.630859375}\n",
            "Terminating because of early stopping!\n"
          ]
        }
      ],
      "source": [
        "tval=training_loop(model,lr_scheduler,optimizer,device,num_epochs=num_train_epochs,train_dataloader=trainDL,eval_dataloader=valDL,total_training_steps=total_steps,completed_steps=0,log_steps=1000,t_log=\"curr_train_log.csv\",eval_steps=28937,e_log=\"eval_log.csv\",early_stopper=es,output_dir=(sync_dir+\"currOPT125MV1\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjhMyyU61Zlg",
        "colab": {
          "referenced_widgets": [
            "b00419a28a9d4615b3b9e4948742bcfb"
          ]
        },
        "outputId": "586579eb-a96c-4d36-d30e-6af4c3f970f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28937\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b00419a28a9d4615b3b9e4948742bcfb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/909795 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batches created: 28668\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "evaluate() missing 1 required positional argument: 'device'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(tval)\n\u001b[1;32m      2\u001b[0m testDL\u001b[38;5;241m=\u001b[39mcreate_tokenized_batches(test,tokenizer,context_length,batch_size,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtestDL.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m tloss,tppl\u001b[38;5;241m=\u001b[39m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestDL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m resultDict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT-value\u001b[39m\u001b[38;5;124m\"\u001b[39m:tval,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:tloss\u001b[38;5;241m.\u001b[39mitem(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining perplexity\u001b[39m\u001b[38;5;124m\"\u001b[39m:tppl}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(resultDict)\n",
            "\u001b[0;31mTypeError\u001b[0m: evaluate() missing 1 required positional argument: 'device'"
          ]
        }
      ],
      "source": [
        "print(tval) #can refactor this cell into a testing function\n",
        "testDL=create_tokenized_batches(test,tokenizer,context_length,batch_size,\"testDL.csv\")\n",
        "tloss,tppl=evaluate(model,testDL,device)\n",
        "resultDict={\"T-value\":tval,\"test loss\":tloss.item(),\"test perplexity\":tppl} #as at start of run step was returned as T-val instead of completed step , will have to multiply by epoch to get true num\n",
        "print(resultDict)\n",
        "with open(\"results.csv\", 'w', newline='') as file:\n",
        "  writer = csv.DictWriter(file, fieldnames=resultDict.keys())\n",
        "  writer.writeheader()\n",
        "  writer.writerow(resultDict.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bkdnL89oOvw"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}