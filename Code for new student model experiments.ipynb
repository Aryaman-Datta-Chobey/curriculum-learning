{"cells":[{"cell_type":"markdown","metadata":{"id":"jOdJoVQYdzCi"},"source":["#Model Hub Login config (ToDO)"]},{"cell_type":"markdown","metadata":{"id":"vrfxv4qBd4kb"},"source":["Import /install libraries"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21705,"status":"ok","timestamp":1704888031451,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":480},"id":"vPJ_SnahHwmO","outputId":"62bdfc01-2335-476d-813c-66231e02bf81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: dill, multiprocess, datasets\n","Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.0)\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.20.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n","Collecting accelerate>=0.20.3 (from transformers[torch])\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.25.0\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"]}],"source":["!pip install datasets\n","!pip install tokenizers\n","!pip install transformers[torch]\n","!pip install tqdm"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9YpmHeYFapj","executionInfo":{"status":"ok","timestamp":1704875882949,"user_tz":480,"elapsed":22573,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"}},"outputId":"6519e26a-c38b-47e0-d574-05e2d00fc7ca"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"ki0EGyQd5mj8"},"source":["# Data sets\n","Load data from CSV files for test and dev as well as surpisal sorted csv file for train\n","\n","custom dataset class? https://www.youtube.com/watch?v=PXOzkkB5eH0"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":1469,"status":"ok","timestamp":1704888036593,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":480},"id":"7OMmMZgd7Wlu"},"outputs":[],"source":["import datasets\n","from datasets import load_dataset, DatasetDict\n","def create_ds_dict(ds_dir):\n","  '''args:str ds_dir: fpath to directory with sentence CSV files\n","          str train_type: rand or curriculum\n","          lst int seeds: list of seeds for shuffling dataset splits\n","     return DatasetDict ds_dict where {k-->ds_split:v-->ds with sentIDs and sentences}'''\n","  #datFiles={\"train\":(ds_dir+\"babylm_sent_train.csv\"),\"validation\":(ds_dir+\"babylm_sent_dev.csv\"),\"test\":(ds_dir+\"babylm_sent_test.csv\")}\n","  csv_dict=load_dataset(\"csv\", data_files=\"/content/curriculum_lstmrev_sumsurp_root2.0_T150000.csv\", sep=\"\\037\") #csv_dict=load_dataset(\"csv\", data_files=datFiles, sep=\"\\037\") #load train test dev split into a Datasetdictionary\n","  ds_dict=DatasetDict()\n","  for k,v in csv_dict.column_names.items(): #remove unecessary columns\n","    ds_dict=csv_dict.remove_columns([col for col in v if col!='sentid'and col!='sentence'])\n","  #shuffle data , according to train_type\n","  '''if train_type==\"rand\":\n","    print(\"pre-shuffle train:\",ds_dict[\"train\"][\"sentence\"][1])\n","    ds_dict[\"train\"].shuffle(seed)\n","    print(\"post-shuffle train\",ds_dict[\"train\"][\"sentence\"][1])\n","  print(\"pre-shuffle valid:\",ds_dict[\"validation\"][\"sentence\"][1])\n","  ds_dict[\"validation\"].shuffle(seed)\n","  print(\"post-shuffle valid:\",ds_dict[\"validation\"][\"sentence\"][1])\n","  print(\"pre-shuffle test:\",ds_dict[\"test\"][\"sentence\"][1])\n","  ds_dict[\"test\"].shuffle(seed)\n","  print(\"post-shuffle test:\",ds_dict[\"test\"][\"sentence\"][1])'''\n","  return ds_dict\n"]},{"cell_type":"code","source":["curr_dict=create_ds_dict('')\n","print(curr_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"id":"qOevEXjTHPEe","executionInfo":{"status":"error","timestamp":1704875938141,"user_tz":480,"elapsed":620,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"}},"outputId":"d8b6799f-32f2-4300-ecae-f677254608e7"},"execution_count":10,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"Unable to find '/content/curriculum_lstmrev_sumsurp_root2.0_T150000.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-4dec2fbafbd1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurr_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_ds_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-f5d536384f55>\u001b[0m in \u001b[0;36mcreate_ds_dict\u001b[0;34m(ds_dir)\u001b[0m\n\u001b[1;32m      7\u001b[0m      return DatasetDict ds_dict where {k-->ds_split:v-->ds with sentIDs and sentences}'''\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#datFiles={\"train\":(ds_dir+\"babylm_sent_train.csv\"),\"validation\":(ds_dir+\"babylm_sent_dev.csv\"),\"test\":(ds_dir+\"babylm_sent_test.csv\")}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mcsv_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/curriculum_lstmrev_sumsurp_root2.0_T150000.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\037\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#csv_dict=load_dataset(\"csv\", data_files=datFiles, sep=\"\\037\") #load train test dev split into a Datasetdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mds_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDatasetDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#remove unecessary columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2522\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2523\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2524\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2193\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   2196\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m         ).get_module()\n\u001b[0m\u001b[1;32m   1737\u001b[0m     \u001b[0;31m# Try locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0mpatterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         data_files = DataFilesDict.from_patterns(\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatterns_for_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             out[key] = (\n\u001b[0;32m--> 689\u001b[0;31m                 DataFilesList.from_patterns(\n\u001b[0m\u001b[1;32m    690\u001b[0m                     \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                 data_files.extend(\n\u001b[0;32m--> 594\u001b[0;31m                     resolve_pattern(\n\u001b[0m\u001b[1;32m    595\u001b[0m                         \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                         \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mallowed_extensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" with any supported extension {list(allowed_extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/content/curriculum_lstmrev_sumsurp_root2.0_T150000.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"KHPAlTqG5tCp"},"source":["#Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"NWt2FwxUJZdW"},"source":["###Loading pre-trained BPE"]},{"cell_type":"markdown","metadata":{"id":"QLo9l5MLJdNA"},"source":["### Tokenizer components, hugging face links for component options and , GPT 2 BPE specification\n","1. [Normalization](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers) ( any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.):\n","  *   GPT2 BPE does not use a normalizer\n","2. [Pre-tokenization](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers) (splitting the input into words):\n","  *   GPT2 BPE is not dependent on significant  pre-tokenization\n","3. Running the input through the [model](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models) (using the pre-tokenized words to produce a sequence of tokens) and train tokenizer using model specific [trainer](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers):\n","  *   GPT2 BPE uses the BPE model\n","4. [Post-processing](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors) (adding the special tokens of the tokenizer, generating the attention mask and token type IDs):\n","  *   GPT2 BPE uses????\n","5. [Decoding](https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders) (technically not a compondent of tokenization , but the process through which one decodes the output of tokenization):\n","  *   Logically, given a byte level encoder , should use a byte level decoder\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"m6cnJyPrJo6h","executionInfo":{"status":"ok","timestamp":1704888062525,"user_tz":480,"elapsed":145,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"}}},"outputs":[],"source":["#tokenizer related libraries\n","import tokenizers\n","from tokenizers import (\n","    normalizers, #1\n","    Tokenizer,\n","    pre_tokenizers, #2\n","    models, #3\n","    trainers,#3\n","    processors, #4\n","    decoders, #5\n",")\n","from transformers import PreTrainedTokenizerFast\n","def create_BPE_tok(sents,save_dir):\n","  '''args: str list sents\n","           str save_dir\n","     returns: PreTrainedTokenizerFast wrapped tokenizer'''\n","  tokenizer = Tokenizer(models.BPE()) #tokenizer objects are intialized with a model argument which specifies with algorithm they will use for tokenization\n","  #GPT-2 does not use a normalizer, so we skip #1 and go directly to #2: pre-tokenization\n","  tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False) #prevents default addition of space char at the beginning of a sentence\n","  #3 passing input to model and training tokenizer\n","  trainer = trainers.BpeTrainer(vocab_size=50272, special_tokens=[\"<|endoftext|>\", '<image>', '</c>', '<PERSON>']) # EOS is apparently the only special token for BPE (rest based on baseline github) , set vocab size according to babyLM baseline model config val\n","  tokenizer.train_from_iterator(sents, trainer=trainer)#tokenizer.train([\"train.txt\"], trainer=trainer) #can use train from iterator to file by file\n","  tokenizer.post_processor = processors.ByteLevel(trim_offsets=False) #4\n","  tokenizer.decoder = decoders.ByteLevel()#5\n","\n","  ###Save (and Push?) Tokenizer\n","  wrapped_tokenizer = PreTrainedTokenizerFast(\n","      tokenizer_object=tokenizer,\n","      bos_token=\"<|endoftext|>\",\n","      eos_token=\"<|endoftext|>\",\n","  )\n","  #usable by HF to recreate this tokenizer\n","  wrapped_tokenizer.save_pretrained(save_dir+\"babyLM10M-byte-level-BPE-tokenizer\")\n","  #tokenizer.push_to_hub(\"babyLM10M-byte-level-BPE-tokenizer\") #usable if logged into HF hub\n","  return wrapped_tokenizer"]},{"cell_type":"markdown","source":["##Load Tokenizer"],"metadata":{"id":"BVdQIGwyH5Yb"}},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":170,"status":"error","timestamp":1704888070899,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":480},"id":"0D1lGMc7-5Sb","colab":{"base_uri":"https://localhost:8080/","height":398},"outputId":"9c340338-57d3-442d-c3fb-30c0c9be7c88"},"outputs":[{"output_type":"error","ename":"HFValidationError","evalue":"Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/OPT 125M checkpoints/babyLM10M-byte-level-BPE-tokenizer'. Use `repo_type` argument if needed.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-32fbf08e2ec8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#tokenizer=torch.load(\"path to tokenizer\") #likely wont work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Code for loading tokenizer from HF wrapper (more likely to work)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/OPT 125M checkpoints/babyLM10M-byte-level-BPE-tokenizer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Could specify path to a checkpoint or a tokenizer directory (requires that tokenizer was saved using HF)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         ):\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/OPT 125M checkpoints/babyLM10M-byte-level-BPE-tokenizer'. Use `repo_type` argument if needed."]}],"source":["from transformers import AutoTokenizer\n","# code for loading tokenizer from pytorch object\n","#tokenizer=torch.load(\"path to tokenizer\") #likely wont work\n","#Code for loading tokenizer from HF wrapper (more likely to work)\n","tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/OPT 125M checkpoints/babyLM10M-byte-level-BPE-tokenizer\") #Could specify path to a checkpoint or a tokenizer directory (requires that tokenizer was saved using HF)"]},{"cell_type":"markdown","metadata":{"id":"vt52DnknQbOr"},"source":["# Get Batches (data loading)\n","1.   Order sentences:Shuffled during dataset laoding (depending on whether currOPT or randOPT)\n","2.   Tokenize and truncate sentences according to context length\n","3.   use data collator to take tokenized list and dynamically pad (pad to longest) as per batch size (32 sequences)\n"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":5328,"status":"ok","timestamp":1704888088201,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":480},"id":"sYqGLqxr60Io"},"outputs":[],"source":["import torch\n","from transformers import DataCollatorForLanguageModeling\n","import csv\n","def dynamically_pad_pretokenized(tokenized_batch,tokenizer):\n","  '''arg: batch of inputIds and corresponding tokenizer\n","     return: dynamically padded batch of pt tensors for causal LM'''\n","  tokenizer.pad_token = tokenizer.eos_token\n","  data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False , return_tensors=\"pt\")\n","  tensor=data_collator(tokenized_batch)\n","  return tensor"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1704888088202,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":480},"id":"uomYF4y48PrW"},"outputs":[],"source":["def collate_and_log_batch(tokenized_batch,tokenizer,batches_so_far,log_file):\n","  '''arg: batch of inputIds and corresponding tokenizer , log_file\n","     log: info about tensor dimensions into a csv\n","     return: dynamically padded batch of pt tensors for causal LM'''\n","  #print(len(tokenized_batch))\n","  collated_batch=dynamically_pad_pretokenized(tokenized_batch,tokenizer)\n","  #print(len(collated_batch))\n","  fields = collated_batch.keys()\n","\n","  ###Logging#####\n","  if batches_so_far==0:# initialize tensor dim log csv with 1st tensor\n","      with open(log_file, 'w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['Key', 'Size 1', 'Size 2'])\n","        for k, v in collated_batch.items():\n","          writer.writerow([k, v.size(0), v.size(1)])\n","  else:\n","      with open(log_file, 'a', newline='') as file:  # Open the file in append mode\n","            writer = csv.writer(file)\n","            for k, v in collated_batch.items():\n","              writer.writerow([k, v.size(0), v.size(1)])\n","  return collated_batch"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":135,"status":"ok","timestamp":1704888089468,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":480},"id":"xTTNMy7733ns"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","def create_tokenized_batches(sents,tokenizer,context_length,batch_size,log_file):\n","  '''Args: str list sents: ordered list of sentences to iterate over\n","           tokenizer\n","           context_length: num of previous tokens that can be used to predict next token in pretraining\n","           batch_size: num sequences of len<=context_length that can be processed by model in a step\n","           log_file: CSV file to log tensor dimensions to\n","      return:array of pt tensors containing dynamically padded batches'''\n","  collated_batches=[]\n","  batch=[]\n","  for count,sent in tqdm(enumerate(sents, start=1),total=len(sents)):\n","    outputs = tokenizer(\n","        sent,\n","        truncation=True, #chunk sentence according to context length if too long\n","        max_length=context_length,\n","        return_overflowing_tokens=True,\n","        return_length=True,\n","    )\n","    for input_id in outputs[\"input_ids\"]:#input chunks count as seperate sequences\n","      if len(batch)<batch_size:\n","        batch.append(input_id)\n","      else: #batch_size reached ,  collate into tensor\n","        collated_batch=collate_and_log_batch(batch,tokenizer,len(collated_batches),log_file)\n","        collated_batches.append(collated_batch)\n","        #print(len(batch))\n","        batch=[]\n","        batch.append(input_id)\n","  #add last batch (potentially smaller than batch size)\n","  collated_batch=collate_and_log_batch(batch,tokenizer,len(collated_batches),log_file)\n","  collated_batches.append(collated_batch)\n","  print(\"batches created:\",len(collated_batches))\n","  return collated_batches"]},{"cell_type":"code","source":["curr_sents=curr_dict[\"train\"][\"sentence\"]\n","curr_dl=create_tokenized_batches(curr_sents,tokenizer,128,32,\"root2_batch_log.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"id":"9aiNP4d7IN2a","executionInfo":{"status":"error","timestamp":1704875923833,"user_tz":480,"elapsed":333,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"}},"outputId":"c7127400-ed86-40e5-f589-c2912a2215c9"},"execution_count":9,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'curr_dict' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-bbeae6f2446c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurr_sents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcurr_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_tokenized_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_sents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"root2_batch_log.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'curr_dict' is not defined"]}]},{"cell_type":"code","source":["print(tokenizer.tokenize(curr_sents[0]))\n","print(tokenizer.tokenize(curr_sents[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g4xC9CmWP-hG","executionInfo":{"status":"ok","timestamp":1689117732275,"user_tz":420,"elapsed":348,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"}},"outputId":"1fa3f6ff-4dca-4960-e03f-43c06e4f7ea3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['-', 'ĠWhat', '?']\n","['I', 'Ġdon', \"'t\", 'Ġknow', '.']\n"]}]},{"cell_type":"markdown","metadata":{"id":"Sw51StwP5vA0"},"source":["#OPT models"]},{"cell_type":"markdown","metadata":{"id":"NafHz4P-_lYM"},"source":["##Configuration and initialization\n","Code for intializing 5 OPT models for student model experients. The model in the orignal paper  closely replicates OPT125M architecture , using details from [(Zhang et al., 2022) ](https://https://arxiv.org/pdf/2205.01068.pdf) and [babyLM OPT125M vaseline](https://huggingface.co/babylm/opt-125m-strict-small/blob/main/config.json).  In new experiments we additionally replicate OPT350M from zhang et Al. and design 3 smaller OPT style models at 55M , 24M and 10M parameters . See further details on the 15 params at huggingface OPTconfig documentation\n"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"N4MKyJib_nZz","executionInfo":{"status":"ok","timestamp":1704888095897,"user_tz":480,"elapsed":126,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"}}},"outputs":[],"source":["from transformers.models.opt import OPTConfig\n","from transformers.models.opt.modeling_opt import OPTForCausalLM\n","def init_OPT_125M_CLM(): #orignal student model\n","  '''initialize a OPT 125M model with a 0.2M param causal language modelling head for pre-training , according configuration params rationalized in comments'''\n","  #Set config params\n","  config=OPTConfig()\n","  print(config) #check defaults\n","  config.vocab_size=50272 #vocab size used in babylm baseline\n","  config.hidden_size=768 #See dLayers , table 1 zhang et Al.\n","  config.num_hidden_layers=12 #See #L , table 1 zhang et Al.\n","  config.ffn_dim=3072 #see OPT125m baseline\n","  config.num_attention_heads=12 #See #H , table 1 zhang et Al.\n","  config.activation_function=\"relu\"#see zhang et al\n","  config.max_position_embeddings=2048 #see OPT125m baseline\n","  config.do_layer_norm_before=True #see OPT125m baseline\n","  config.word_embed_proj_dim=768 #generally equal to hidden size , intialized accoring to OPT125M baseline\n","  config.dropout=0.1 # see Zhang et Al : \"We use a dropout of 0.1 throughout, but we do not apply any dropout to embeddings.\"\n","  config.attention_dropout=0.0 #see OPT125M baseline\n","  config.init_std=0.02 #Intialized according to OPT125M baseline however potnetially signifcant differences from zhang et Al: “For weight initialization ….. using a normal distribution with zero mean and standard deviation of 0.006.”” Standard deviation for output layers are scaled by a 1.0/ √ 2L term where L is the total number of layers. ”\n","  config.use_cache=True #see OPT125M baseline\n","  config.enable_bias=True #see OPT125M baseline\n","  config.layer_norm_elementwise_affine=True #see OPT125M baseline\n","  print(config) #check updates\n","  #intialize OPTm model with a causalLM head for training\n","  OPTCLM=OPTForCausalLM(config)\n","  #check architecture\n","  print(OPTCLM)\n","  model_size = sum(t.numel() for t in OPTCLM.parameters())\n","  print(f\"OPT size: {model_size/1000**2:.1f}M parameters\")\n","  return OPTCLM\n","\n","def init_OPT_350M_CLM():#model from zhang et AL larger than orignal student\n","  '''initialize a OPT ?M model with a 0.2M param causal language modelling head for pre-training , according configuration params rationalized in comments'''\n","  #Set config params\n","  config=OPTConfig()\n","  print(config) #check defaults\n","  config.vocab_size=50272 #vocab size used in babylm baseline\n","  config.hidden_size=1024 #See dLayers , table 1 zhang et Al.\n","  config.num_hidden_layers= 24 #See #L , table 1 zhang et Al.\n","  config.ffn_dim=3072 #see OPT125m baseline\n","  config.num_attention_heads=16 #See #H , table 1 zhang et Al.\n","  config.activation_function=\"relu\"#see zhang et al\n","  config.max_position_embeddings=2048 #see OPT125m baseline\n","  config.do_layer_norm_before=True #see OPT125m baseline\n","  config.word_embed_proj_dim=1024 #generally equal to hidden size , intialized accoring to OPT125M baseline\n","  config.dropout=0.1 # see Zhang et Al : \"We use a dropout of 0.1 throughout, but we do not apply any dropout to embeddings.\"\n","  config.attention_dropout=0.0 #see OPT125M baseline\n","  config.init_std=0.02 #Intialized according to OPT125M baseline however potnetially signifcant differences from zhang et Al: “For weight initialization ….. using a normal distribution with zero mean and standard deviation of 0.006.”” Standard deviation for output layers are scaled by a 1.0/ √ 2L term where L is the total number of layers. ”\n","  config.use_cache=True #see OPT125M baseline\n","  config.enable_bias=True #see OPT125M baseline\n","  config.layer_norm_elementwise_affine=True #see OPT125M baseline\n","  print(config) #check updates\n","  #intialize OPTm model with a causalLM head for training\n","  OPTCLM=OPTForCausalLM(config)\n","  #check architecture\n","  print(OPTCLM)\n","  model_size = sum(t.numel() for t in OPTCLM.parameters())\n","  print(f\"OPT size: {model_size/1000**2:.1f}M parameters\")\n","  return OPTCLM\n","\n","def init_OPT_55M_CLM():\n","  '''initialize 55 million parameter OPT style model with a 0.2M param causal language modelling head for pre-training , according configuration params rationalized in comments'''\n","  #Set config params\n","  config=OPTConfig()\n","  print(config) #check defaults\n","  config.vocab_size=50272 #vocab size used in babylm baseline\n","  config.hidden_size=384 #See dLayers , table 1 zhang et Al.\n","  config.num_hidden_layers= 12 #See #L , table 1 zhang et Al.\n","  config.ffn_dim=3072 #see OPT125m baseline\n","  config.num_attention_heads=6 #See #H , table 1 zhang et Al.\n","  config.activation_function=\"relu\"#see zhang et al\n","  config.max_position_embeddings=2048 #see OPT125m baseline\n","  config.do_layer_norm_before=True #see OPT125m baseline\n","  config.word_embed_proj_dim=384 #generally equal to hidden size , intialized accoring to OPT125M baseline\n","  config.dropout=0.1 # see Zhang et Al : \"We use a dropout of 0.1 throughout, but we do not apply any dropout to embeddings.\"\n","  config.attention_dropout=0.0 #see OPT125M baseline\n","  config.init_std=0.02 #Intialized according to OPT125M baseline however potnetially signifcant differences from zhang et Al: “For weight initialization ….. using a normal distribution with zero mean and standard deviation of 0.006.”” Standard deviation for output layers are scaled by a 1.0/ √ 2L term where L is the total number of layers. ”\n","  config.use_cache=True #see OPT125M baseline\n","  config.enable_bias=True #see OPT125M baseline\n","  config.layer_norm_elementwise_affine=True #see OPT125M baseline\n","  print(config) #check updates\n","  #intialize OPTm model with a causalLM head for training\n","  OPTCLM=OPTForCausalLM(config)\n","  #check architecture\n","  print(OPTCLM)\n","  model_size = sum(t.numel() for t in OPTCLM.parameters())\n","  print(f\"OPT size: {model_size/1000**2:.1f}M parameters\")\n","  return OPTCLM\n","\n","def init_OPT_24M_CLM():\n","  '''initialize 24 million parameter OPT style model with a 0.2M param causal language modelling head for pre-training , according configuration params rationalized in comments'''\n","  #Set config params\n","  config=OPTConfig()\n","  print(config) #check defaults\n","  config.vocab_size=50272 #vocab size used in babylm baseline\n","  config.hidden_size=256 #See dLayers , table 1 zhang et Al.\n","  config.num_hidden_layers= 6 #See #L , table 1 zhang et Al.\n","  config.ffn_dim=3072 #see OPT125m baseline\n","  config.num_attention_heads=4 #See #H , table 1 zhang et Al.\n","  config.activation_function=\"relu\"#see zhang et al\n","  config.max_position_embeddings=2048 #see OPT125m baseline\n","  config.do_layer_norm_before=True #see OPT125m baseline\n","  config.word_embed_proj_dim=246 #generally equal to hidden size , intialized accoring to OPT125M baseline\n","  config.dropout=0.1 # see Zhang et Al : \"We use a dropout of 0.1 throughout, but we do not apply any dropout to embeddings.\"\n","  config.attention_dropout=0.0 #see OPT125M baseline\n","  config.init_std=0.02 #Intialized according to OPT125M baseline however potnetially signifcant differences from zhang et Al: “For weight initialization ….. using a normal distribution with zero mean and standard deviation of 0.006.”” Standard deviation for output layers are scaled by a 1.0/ √ 2L term where L is the total number of layers. ”\n","  config.use_cache=True #see OPT125M baseline\n","  config.enable_bias=True #see OPT125M baseline\n","  config.layer_norm_elementwise_affine=True #see OPT125M baseline\n","  print(config) #check updates\n","  #intialize OPTm model with a causalLM head for training\n","  OPTCLM=OPTForCausalLM(config)\n","  #check architecture\n","  print(OPTCLM)\n","  model_size = sum(t.numel() for t in OPTCLM.parameters())\n","  print(f\"OPT size: {model_size/1000**2:.1f}M parameters\")\n","  return OPTCLM\n","\n","def init_OPT_10M_CLM():\n","  '''initialize 10 million parameter OPT style model with a 0.2M param causal language modelling head for pre-training , according configuration params rationalized in comments'''\n","  #Set config params\n","  config=OPTConfig()\n","  print(config) #check defaults\n","  config.vocab_size=50272 #vocab size used in babylm baseline\n","  config.hidden_size=128 #See dLayers , table 1 zhang et Al.\n","  config.num_hidden_layers= 4 #See #L , table 1 zhang et Al.\n","  config.ffn_dim=3072 #see OPT125m baseline\n","  config.num_attention_heads=4 #See #H , table 1 zhang et Al.\n","  config.activation_function=\"relu\"#see zhang et al\n","  config.max_position_embeddings=2048 #see OPT125m baseline\n","  config.do_layer_norm_before=True #see OPT125m baseline\n","  config.word_embed_proj_dim=128 #generally equal to hidden size , intialized accoring to OPT125M baseline\n","  config.dropout=0.1 # see Zhang et Al : \"We use a dropout of 0.1 throughout, but we do not apply any dropout to embeddings.\"\n","  config.attention_dropout=0.0 #see OPT125M baseline\n","  config.init_std=0.02 #Intialized according to OPT125M baseline however potnetially signifcant differences from zhang et Al: “For weight initialization ….. using a normal distribution with zero mean and standard deviation of 0.006.”” Standard deviation for output layers are scaled by a 1.0/ √ 2L term where L is the total number of layers. ”\n","  config.use_cache=True #see OPT125M baseline\n","  config.enable_bias=True #see OPT125M baseline\n","  config.layer_norm_elementwise_affine=True #see OPT125M baseline\n","  print(config) #check updates\n","  #intialize OPTm model with a causalLM head for training\n","  OPTCLM=OPTForCausalLM(config)\n","  #check architecture\n","  print(OPTCLM)\n","  model_size = sum(t.numel() for t in OPTCLM.parameters())\n","  print(f\"OPT size: {model_size/1000**2:.1f}M parameters\")\n","  return OPTCLM"]},{"cell_type":"code","source":["model=init_OPT_10M_CLM() #Run this cell with init_OPTM_?M_CLM() to see a breakdown of parameter counts and diagram of architecture in comparison to OPT125M"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZtrkuRDagMTM","executionInfo":{"status":"ok","timestamp":1704888145116,"user_tz":480,"elapsed":392,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"}},"outputId":"c6ee7e1b-24be-4f33-af09-f0aea2a49ae3"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["OPTConfig {\n","  \"_remove_final_layer_norm\": false,\n","  \"activation_function\": \"relu\",\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 2,\n","  \"do_layer_norm_before\": true,\n","  \"dropout\": 0.1,\n","  \"enable_bias\": true,\n","  \"eos_token_id\": 2,\n","  \"ffn_dim\": 3072,\n","  \"hidden_size\": 768,\n","  \"init_std\": 0.02,\n","  \"layer_norm_elementwise_affine\": true,\n","  \"layerdrop\": 0.0,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"opt\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.35.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50272,\n","  \"word_embed_proj_dim\": 768\n","}\n","\n","OPTConfig {\n","  \"_remove_final_layer_norm\": false,\n","  \"activation_function\": \"relu\",\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 2,\n","  \"do_layer_norm_before\": true,\n","  \"dropout\": 0.1,\n","  \"enable_bias\": true,\n","  \"eos_token_id\": 2,\n","  \"ffn_dim\": 3072,\n","  \"hidden_size\": 128,\n","  \"init_std\": 0.02,\n","  \"layer_norm_elementwise_affine\": true,\n","  \"layerdrop\": 0.0,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"opt\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.35.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50272,\n","  \"word_embed_proj_dim\": 128\n","}\n","\n","OPTForCausalLM(\n","  (model): OPTModel(\n","    (decoder): OPTDecoder(\n","      (embed_tokens): Embedding(50272, 128, padding_idx=1)\n","      (embed_positions): OPTLearnedPositionalEmbedding(2050, 128)\n","      (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-3): 4 x OPTDecoderLayer(\n","          (self_attn): OPTAttention(\n","            (k_proj): Linear(in_features=128, out_features=128, bias=True)\n","            (v_proj): Linear(in_features=128, out_features=128, bias=True)\n","            (q_proj): Linear(in_features=128, out_features=128, bias=True)\n","            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n","          )\n","          (activation_fn): ReLU()\n","          (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=128, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=128, bias=True)\n","          (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): Linear(in_features=128, out_features=50272, bias=False)\n",")\n","OPT size: 10.1M parameters\n"]}]},{"cell_type":"markdown","source":["## RoBERTA Style Models"],"metadata":{"id":"ExjQjYZ-QqbE"}},{"cell_type":"code","source":["from transformers import RobertaConfig\n","from transformers  import RobertaForMaskedLM\n","def init_Roberta_125M_MLM(): #RoBERTA base: Masked language model comparable to orignal student OPT 125M\n","  '''initialize a Roberta base model with a 0.2M param masked language modelling head for pre-training , according configuration params rationalized in comments'''\n","  #Set config params\n","  config=RobertaConfig()\n","  print(config) #check defaults\n","  config.vocab_size=50265 #RoBERTA default\n","  config.hidden_size=768\n","  config.num_hidden_layers=12\n","  config.ffn_dim=3072 #intermediate_size\n","  config.num_attention_heads=12\n","  config.activation_function=\"gelu\"#hidden_act\n","  config.max_position_embeddings=512\n","  config.dropout=0.1 # hidden_dropout_prob\n","  config.attention_dropout=0.0 #attention_probs_dropout_prob\n","  config.init_std=0.02 #initializer_range\n","  config.layer_norm_elementwise_affine=True #layer_norm_eps (float, optional, defaults to 1e-12) — The epsilon used by the layer normalization layers.\n","  print(config) #check updates\n","   #intialize RoBERTA model with a maskedLM head for training\n","  RobertaMLM=RobertaForMaskedLM(config)\n","  #check architecture\n","  print( RobertaMLM)\n","  model_size = sum(t.numel() for t in  RobertaMLM.parameters())\n","  print(f\"Roberta size: {model_size/1000**2:.1f}M parameters\")\n","  return  RobertaMLM\n","\n","def init_Roberta_305M_MLM(): #RoBERTA Large: Masked language model parameter scale comparable to  OPT 350M\n","  '''initialize a Roberta large model with a 0.2M param masked language modelling head for pre-training , according configuration params rationalized in comments'''\n","  #Set config params\n","  config=RobertaConfig()\n","  print(config) #check defaults\n","  config.vocab_size=50265 #RoBERTA default\n","  config.hidden_size=1024\n","  config.num_hidden_layers=24\n","  config.ffn_dim=3072 #intermediate_size\n","  config.num_attention_heads=16\n","  config.activation_function=\"gelu\"#hidden_act\n","  config.max_position_embeddings=512\n","  config.dropout=0.1 # hidden_dropout_prob\n","  config.attention_dropout=0.0 #attention_probs_dropout_prob\n","  config.init_std=0.02 #initializer_range\n","  config.layer_norm_elementwise_affine=True #layer_norm_eps (float, optional, defaults to 1e-12) — The epsilon used by the layer normalization layers.\n","  print(config) #check updates\n","  #intialize RoBERTA model with a maskedLM head for training\n","  RobertaMLM=RobertaForMaskedLM(config)\n","  #check architecture\n","  print( RobertaMLM)\n","  model_size = sum(t.numel() for t in  RobertaMLM.parameters())\n","  print(f\"Roberta size: {model_size/1000**2:.1f}M parameters\")\n","  return  RobertaMLM\n","\n","def init_Roberta_55M_MLM(): #RoBERTA Large: Masked language model parameter scale comparable to  OPT 350M\n","  '''initialize a Roberta style model with a 0.2M param masked language modelling head for pre-training , according configuration params rationalized in comments'''\n","  #Set config params\n","  config=RobertaConfig()\n","  print(config) #check defaults\n","  config.vocab_size=50265 #RoBERTA default\n","  config.hidden_size=384\n","  config.num_hidden_layers=12\n","  config.ffn_dim=3072 #intermediate_size\n","  config.num_attention_heads=6\n","  config.activation_function=\"gelu\"#hidden_act\n","  config.max_position_embeddings=512\n","  config.dropout=0.1 # hidden_dropout_prob\n","  config.attention_dropout=0.0 #attention_probs_dropout_prob\n","  config.init_std=0.02 #initializer_range\n","  config.layer_norm_elementwise_affine=True #layer_norm_eps (float, optional, defaults to 1e-12) — The epsilon used by the layer normalization layers.\n","  print(config) #check updates\n","  #intialize RoBERTA model with a maskedLM head for training\n","  RobertaMLM=RobertaForMaskedLM(config)\n","  #check architecture\n","  print( RobertaMLM)\n","  model_size = sum(t.numel() for t in  RobertaMLM.parameters())\n","  print(f\"Roberta size: {model_size/1000**2:.1f}M parameters\")\n","  return  RobertaMLM\n","\n","def init_Roberta_24M_MLM(): #RoBERTA Large: Masked language model parameter scale comparable to  OPT 350M\n","  '''initialize a Roberta style model with a 0.2M param masked language modelling head for pre-training , according configuration params rationalized in comments'''\n","  #Set config params\n","  config=RobertaConfig()\n","  print(config) #check defaults\n","  config.vocab_size=50265 #RoBERTA default\n","  config.hidden_size=256\n","  config.num_hidden_layers=6\n","  config.ffn_dim=3072 #intermediate_size\n","  config.num_attention_heads=4\n","  config.activation_function=\"gelu\"#hidden_act\n","  config.max_position_embeddings=256\n","  config.dropout=0.1 # hidden_dropout_prob\n","  config.attention_dropout=0.0 #attention_probs_dropout_prob\n","  config.init_std=0.02 #initializer_range\n","  config.layer_norm_elementwise_affine=True #layer_norm_eps (float, optional, defaults to 1e-12) — The epsilon used by the layer normalization layers.\n","  print(config) #check updates\n","  #intialize RoBERTA model with a maskedLM head for training\n","  RobertaMLM=RobertaForMaskedLM(config)\n","  #check architecture\n","  print( RobertaMLM)\n","  model_size = sum(t.numel() for t in  RobertaMLM.parameters())\n","  print(f\"Roberta size: {model_size/1000**2:.1f}M parameters\")\n","  return  RobertaMLM\n","\n","def init_Roberta_10M_MLM(): #RoBERTA Large: Masked language model parameter scale comparable to  OPT 350M\n","  '''initialize a Roberta style model with a 0.2M param masked language modelling head for pre-training , according configuration params rationalized in comments'''\n","  #Set config params\n","  config=RobertaConfig()\n","  print(config) #check defaults\n","  config.vocab_size=50265 #RoBERTA default\n","  config.hidden_size=128\n","  config.num_hidden_layers=4\n","  config.ffn_dim=3072 #intermediate_size\n","  config.num_attention_heads=4\n","  config.activation_function=\"gelu\"#hidden_act\n","  config.max_position_embeddings=128\n","  config.dropout=0.1 # hidden_dropout_prob\n","  config.attention_dropout=0.0 #attention_probs_dropout_prob\n","  config.init_std=0.02 #initializer_range\n","  config.layer_norm_elementwise_affine=True #layer_norm_eps (float, optional, defaults to 1e-12) — The epsilon used by the layer normalization layers.\n","  print(config) #check updates\n","  #intialize RoBERTA model with a maskedLM head for training\n","  RobertaMLM=RobertaForMaskedLM(config)\n","  #check architecture\n","  print( RobertaMLM)\n","  model_size = sum(t.numel() for t in  RobertaMLM.parameters())\n","  print(f\"Roberta size: {model_size/1000**2:.1f}M parameters\")\n","  return  RobertaMLM"],"metadata":{"id":"1nONmc2IQ05l","executionInfo":{"status":"ok","timestamp":1704890417340,"user_tz":480,"elapsed":165,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["model=init_Roberta_10M_MLM() #Run this cell with init_OPTM_?M_CLM() to see a breakdown of parameter counts and diagram of architecture in comparison to OPT125M"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cMqsdvKTUw4F","executionInfo":{"status":"ok","timestamp":1704890427546,"user_tz":480,"elapsed":485,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"}},"outputId":"1cdd240f-3c1a-4628-bee4-84837f7ec259"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["RobertaConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","RobertaConfig {\n","  \"activation_function\": \"gelu\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"dropout\": 0.1,\n","  \"eos_token_id\": 2,\n","  \"ffn_dim\": 3072,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 128,\n","  \"init_std\": 0.02,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_elementwise_affine\": true,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 4,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.35.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","RobertaForMaskedLM(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 128, padding_idx=1)\n","      (position_embeddings): Embedding(128, 128, padding_idx=1)\n","      (token_type_embeddings): Embedding(2, 128)\n","      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-3): 4 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=128, out_features=128, bias=True)\n","              (key): Linear(in_features=128, out_features=128, bias=True)\n","              (value): Linear(in_features=128, out_features=128, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=128, out_features=128, bias=True)\n","              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=128, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=128, bias=True)\n","            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): RobertaLMHead(\n","    (dense): Linear(in_features=128, out_features=128, bias=True)\n","    (layer_norm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n","    (decoder): Linear(in_features=128, out_features=50265, bias=True)\n","  )\n",")\n","Roberta size: 9.9M parameters\n"]}]},{"cell_type":"markdown","metadata":{"id":"2mHEdDwq52Fc"},"source":["## Optimizer and Scheduler\n","\n","Both OPT and RoBERTA models have been trained with identical optimizers and learning rate schedulers\n","\n","OPT paper:\"We follow a linear learning\n","rate schedule, warming up from 0 to the maximum\n","learning rate over the first 2000 steps in OPT-175B,\n","or over 375M tokens in our smaller baselines, and\n","decaying down to 10% of the maximum LR over\n","300B tokens. A number of mid-flight changes\n","to LR were also required (see Section 2.5). Our\n","batch sizes range from 0.5M to 4M depending on\n","the model size (see Table 1) and is kept constant\n","throughout the course of training.\"\n","\n","Platanios et al:\n","*   for large experiments with tranformers T=50K (see curriculum hyperparameters) (large dataset was WMT with 4.5M sentences)\n","*   Twarmup  \"is set to 10,000 in these experiments\" => 1/5 of steps\n","*    \"This schedule was proposed in the original\n","Transformer paper (Vaswani et al., 2017)\"\n","\n","Vaswani et al., 2017:\n","* \"We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n","bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n","(3.5 days).\"\n","* Also used linear learning rate \"We used warmup_steps = 4000.\" => small fraction\n","*Also had the same large WMT 4.5M sentence dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZ5t-TySA7Zp"},"outputs":[],"source":["from transformers import get_scheduler\n","from transformers import AdamW\n","def create_optimizer_and_lr_scheduler(total_training_steps):\n","  '''args: int total_training_steps: int epochs*int batches per epoch\n","     returns AdamW optimizer , linear learning rate scheduler (configured as per  OPT paper) '''\n","  optimizer = AdamW(model.parameters(), lr=5e-5,betas=(0.9,0.95),weight_decay=0.1) #Zhang et Al \"We use an AdamW optimizer (Loshchilov andHutter, 2017) with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1\"\n","  lr_scheduler = get_scheduler(\n","      name=\"linear\", #zhang et Al. followed a linear learning rate schedule , midflight changes not relevant to 125M model and  our small dataset\n","      optimizer=optimizer,\n","      num_warmup_steps=int(total_training_steps*0.05), #TODO set as a fraction of training steps=> going with 5%\n","      num_training_steps=total_training_steps\n","   )\n","  return optimizer, lr_scheduler"]},{"cell_type":"markdown","metadata":{"id":"vuF_pA8s6Btc"},"source":["# Evaluation\n","\n","https://huggingface.co/learn/nlp-course/chapter3/4?fw=pt#the-evaluation-loop\n","\"Since we want to evaluate the model regularly on the validation set during training, let’s write a function for that as well. It just runs through the evaluation dataloader and gathers all the losses across processes:\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6qY4-32Hmf_"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","def evaluate(model,eval_dataloader,device):\n","  '''evaluate model to get loss and perplexity'''\n","  model.eval()\n","  losses = []\n","  for step, batch in tqdm(enumerate(eval_dataloader,start=1),total=len(eval_dataloader)):\n","     batch_on_device = {k: v.to(device) for k, v in batch.items()}\n","     with torch.no_grad():\n","          outputs = model(batch_on_device[\"input_ids\"], labels=batch_on_device[\"input_ids\"])\n","     losses.append(outputs.loss)#losses.append(accelerator.gather(outputs.loss))### change\n","     '''if step %100==0:\n","            print({\"step\":step,\"loss\":(outputs.loss).item(),\"loss size\":(outputs.loss).size()})'''\n","  loss = torch.mean(torch.stack(losses))#torch.mean(torch.cat(losses)) cant concat 0 dim tensors\n","  try:\n","      perplexity = torch.exp(loss)\n","  except OverflowError:\n","      perplexity = float(\"inf\")\n","  return loss, perplexity.item()"]},{"cell_type":"markdown","metadata":{"id":"NNZ1hxam-LOg"},"source":["##Early Stopping\n","class adapted with minimal changes from : https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKz4YeGe-QaO"},"outputs":[],"source":["import numpy as np\n","\n","class EarlyStopping(object):\n","    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n","        '''params:str mode: Indicates whether to minimize ('min') or maximize ('max') the monitored metric.\n","                 float min_delta: The minimum change required to qualify as an improvement in the monitored metric.\n","                 int patience: The number of epochs to wait for an improvement before stopping the training process.\n","                 bool percentage: If True, min_delta is interpreted as a percentage change rather than an absolute value.'''\n","        self.mode = mode\n","        self.min_delta = min_delta\n","        self.patience = patience\n","        self.best = None\n","        self.num_bad_epochs = 0\n","        self.is_better = None\n","        self._init_is_better(mode, min_delta, percentage)\n","\n","        # If patience is set to 0, always consider the current metric as the best\n","        if patience == 0:\n","            self.is_better = lambda a, b: True\n","            self.step = lambda a: False\n","\n","    def step(self, metrics):\n","        if self.best is None:\n","            self.best = metrics\n","            return False\n","\n","        # If the metric is NaN, consider it as a signal to stop early\n","        if torch.isnan(metrics):#np.isnan(metrics):# consider using torch.isnan(metrics) :\n","            return True\n","\n","        # Check if the current metric is better than the best\n","        if self.is_better(metrics, self.best):\n","            self.num_bad_epochs = 0\n","            self.best = metrics\n","        else:\n","            self.num_bad_epochs += 1\n","\n","        # If the number of bad epochs exceeds patience, stop early\n","        if self.num_bad_epochs >= self.patience:\n","            print('Terminating because of early stopping!')\n","            return True\n","        return False\n","\n","    def _init_is_better(self, mode, min_delta, percentage):\n","        if mode not in {'min', 'max'}:\n","            raise ValueError('Mode ' + mode + ' is unknown!')\n","        if not percentage:\n","            if mode == 'min':\n","                # Check if a is smaller than (best - min_delta)\n","                self.is_better = lambda a, best: a < best - min_delta\n","            if mode == 'max':\n","                # Check if a is larger than (best + min_delta)\n","                self.is_better = lambda a, best: a > best + min_delta\n","        else:\n","            if mode == 'min':\n","                # Check if a is smaller than (best - (best * min_delta / 100))\n","                self.is_better = lambda a, best: a < best - (best * min_delta / 100)\n","            if mode == 'max':\n","                # Check if a is larger than (best + (best * min_delta / 100))\n","                self.is_better = lambda a, best: a > best + (best * min_delta / 100)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":83},"id":"VdFjT4h5BCVb","outputId":"249070a3-9570-4d03-98da-ef84feadbc33"},"outputs":[{"ename":"NameError","evalue":"name 'train_one_epoch' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 7\u001b[0m       \u001b[43mtrain_one_epoch\u001b[49m(model, data_loader)  \u001b[38;5;66;03m# train the model for one epoch.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m       metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, data_loader_dev)  \u001b[38;5;66;03m# evalution on dev set.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m es\u001b[38;5;241m.\u001b[39mstep(metric):\n","\u001b[0;31mNameError\u001b[0m: name 'train_one_epoch' is not defined"]}],"source":["#Example\n","es = EarlyStopping(patience=5)\n","\n","\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","      train_one_epoch(model, data_loader)  # train the model for one epoch.\n","      metric = eval(model, data_loader_dev)  # evalution on dev set.\n","      if es.step(metric):\n","          break  # early stop criterion is met, we can stop now\n"]},{"cell_type":"markdown","metadata":{"id":"b5IVp-0dhWEj"},"source":["# Logging\n","##TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W7MTa89OhiOj","outputId":"b6e85130-010f-409c-89c9-34fdd27098e9"},"outputs":[{"ename":"IndentationError","evalue":"unexpected indent (1643835284.py, line 6)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[11], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    writer.add_scalars('Training vs. Validation Loss',\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}],"source":["from torch.utils.tensorboard import SummaryWriter\n","writer=SummaryWriter(log_dir)\n","\n","####Log scalars\n"," # Log the running loss averaged per batch\n","            writer.add_scalars('Training vs. Validation Loss',\n","                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n","                            epoch * len(training_loader) + i)\n","####Visualize network\n","# Again, grab a single mini-batch of images\n","dataiter = iter(training_loader)\n","images, labels = dataiter.next()\n","\n","# add_graph() will trace the sample input through your model,\n","# and render it as a graph.\n","writer.add_graph(net, images)\n","writer.flush()"]},{"cell_type":"markdown","metadata":{"id":"nYEW-W3ompxH"},"source":["## CSV writer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mziLykrOmuSL"},"outputs":[],"source":["import csv\n","\n","def log_prog(dicts, fname, curr_step, log_steps):\n","    '''Args:\n","       - list[dict] dicts: Training loop logging entries for a list of batches.\n","       - str fname: Path of the file to append entries to.\n","       - int curr_step: The training step at which the logging is happening.\n","       - int log_steps: This function is called every log_steps steps. curr_step%log_steps ==0\n","       Returns: None'''\n","\n","    try: # avoid inerupting training loop\n","        fields = dicts[0].keys()\n","        if curr_step/log_steps == 1:  # Check if it's the 1st logging instance\n","            with open(fname, 'w', newline='') as file:\n","                writer = csv.DictWriter(file, fieldnames=fields)\n","                writer.writeheader()\n","                writer.writerows(dicts)\n","        else:\n","            with open(fname, 'a', newline='') as file:  # Open the file in append mode\n","                writer = csv.writer(file)\n","                for dictionary in dicts:\n","                  writer.writerow(dictionary.values())\n","    except Exception as e:\n","        print(f\"Logging failed: {e}\")\n","\n","    return\n"]},{"cell_type":"markdown","metadata":{"id":"lSLYqdpz57Ol"},"source":["#Training loop"]},{"cell_type":"markdown","metadata":{"id":"3flbd682i00F"},"source":["The training loop is currently implemented as several helper functions   \n","\n","*   carries out standard stochastic gradient descent pre-training (randOPT)\n","*   supports curriculum learning pre-training by passing appropriate dataloaders as function parameters [curriculum learning consists of a single long epoch instead of several epochs as in SGD]\n","*   Currently only supports Causal Language Modelling pre-training objective [trains autoregressive transformer models such as OPT]\n","*   [TODO] Refactor helper questions so that our training loop can also support Masked Language Modelling Pre-training [needed to pre-train RoBERTA]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fiHKlfLfGVAc"},"outputs":[],"source":["\n","def train_one_step(model,device,batch,optimizer,lr_scheduler):\n","  '''Args: model to train one step (forward and backward pass)\n","                 associated optimizer and scheduler\n","            pt tensor batch to be passed through model\n","     return: current loss and learning rate for logging '''\n","  #Forward Pass\n","  batch_on_device = {k: v.to(device) for k, v in batch.items()}\n","  fpass_output=model(input_ids=batch_on_device[\"input_ids\"],labels=batch_on_device[\"labels\"]) # Forward pass: **batch ??\n","  logits = fpass_output.logits  #Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax):logits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size))\n","  loss =fpass_output.loss #cross-entropy loss: loss (torch.FloatTensor of shape (1,), optional, returned when labels is provided) — Language modeling loss (for next-token prediction).\n","  #update logging vars\n","  curr_loss=loss.item()\n","  curr_lr=lr_scheduler.get_lr()\n","  #backward pass\n","  loss.backward(loss)\n","  torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip the gradients to prevent exploding gradients => used 1.0 to 0.3 clipping in OPT paper\n","  optimizer.step()\n","  lr_scheduler.step()\n","  optimizer.zero_grad()  # Clear gradients from the previous optimization step\n","  return curr_loss , curr_lr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-uwzkIaNFfLc"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","def train_one_epoch(model,lr_scheduler,optimizer,device,epoch,train_dataloader,eval_dataloader,total_training_steps,completed_steps,log_steps,t_log,eval_steps,e_log,early_stopper,spath):\n","  '''Args:  model and associated lr_scheduler and optimizer being trained on device\n","            int epoch being trained\n","            tensor list train and eval dataloaders\n","            int steps\n","            fnames for log_files training and eval\n","            EarlyStopping early_stopper\n","      return  step, boolean converged '''\n","  progress=[]\n","  for step, batch in tqdm(enumerate(train_dataloader, start=1), total=total_training_steps): #iterate with progress bar where  'step'= current batch index, 'batch' contains the input data for the current batch , represent batch 0 as batch 1\n","    loss,lr=train_one_step(model,device,batch,optimizer,lr_scheduler)\n","    completed_steps += 1\n","    prog={\"steps\": completed_steps, \"loss/train\": loss,\"lr\": lr,\"tensor_memory\":torch.cuda.memory_allocated(device)}\n","    progress.append(prog)\n","    if (epoch*step) % (epoch*log_steps) == 0: #LOGGING progress metrics computed on train set every log_steps\n","      log_prog(progress,t_log,step, log_steps)\n","    if ((epoch*step) % (epoch*eval_steps)) == 0:# Perform model evaluation on validation data after every eval_steps, set eval_steps=len(train_dataloader) for per epoch logging (generalizes to currOPT)\n","            eval_loss, perplexity = evaluate(model,eval_dataloader,device)  # Evaluate the model's performance on the evaluation dataset\n","            result=[{\"loss/eval\": eval_loss, \"perplexity\": perplexity}]\n","            log_prog(result,e_log,step,eval_steps)\n","            progress=[]\n","            #####check pointing code######\n","            torch.save({\n","            'epoch': epoch,\n","            'completed_steps':completed_steps,\n","            'model_state_dict': model.state_dict(),\n","            'lr_scheduler': lr_scheduler.state_dict(),\n","            'lr':lr,\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'eval_loss': eval_loss,\n","            }, spath+str(completed_steps)+'.pt') #save to path +name+'step:'+completed_steps+'.pt'\n","            if early_stopper.step(eval_loss):\n","              return step ,True #converged\n","            model.train()  # Set the model back to training mode\n","  return step, False #will continue training unless last epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnoNdLYbETYN"},"outputs":[],"source":["def training_loop(model,lr_scheduler,optimizer,device,num_epochs,train_dataloader,eval_dataloader,total_training_steps,completed_steps,log_steps,t_log,eval_steps,e_log,early_stopper,output_dir):\n","  tval=0\n","  for epoch in range(1,num_epochs+1):\n","    tval,converged=train_one_epoch(model,lr_scheduler,optimizer,device,epoch,train_dataloader,eval_dataloader,total_training_steps,completed_steps,log_steps,t_log,eval_steps,e_log,early_stopper,spath=output_dir)\n","    if converged:\n","       model.save_pretrained(output_dir)\n","       return tval\n","    #else prepare for next epoch\n","    completed_steps=tval\n","    random.shuffle(train_dataloader)\n","    random.shuffle(eval_dataloader)\n","  model.save_pretrained(output_dir)\n","  return tval"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1688918305557,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":420},"id":"YFzgMQdHUbXU","outputId":"cf9d7234-c64a-4fb6-ad16-88d7ca2dfdee"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import random\n","#Initial params\n","random.seed(32) #set seed\n","num_train_epochs=2\n","context_length=128\n","batch_size=32\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)\n","sync_dir='' #'/home/sharedDATA/models/'\n","#directories\n","tok_dir=\"\" #store tokenizer\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"executionInfo":{"elapsed":604,"status":"error","timestamp":1688897743546,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":420},"id":"qGZDBOYGHmS2","outputId":"bdbb4b11-4d39-4c44-ea46-408bb8be8a2c"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/IPython/core/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">interactiveshell.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3553</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run_code</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3550 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> async_ :                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3551 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">await</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">eval</span>(code_obj, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_global_ns, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_ns)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3552 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3553 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>exec(code_obj, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_global_ns, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.user_ns)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3554 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3555 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Reset our crash handler in place</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3556 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>sys.excepthook = old_excepthook                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ModuleNotFoundError: </span>No module named <span style=\"color: #008000; text-decoration-color: #008000\">'pynvml'</span>\n","</pre>\n"],"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/IPython/core/\u001b[0m\u001b[1;33minteractiveshell.py\u001b[0m:\u001b[94m3553\u001b[0m in \u001b[92mrun_code\u001b[0m        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3550 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melif\u001b[0m async_ :                                                             \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3551 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mawait\u001b[0m \u001b[96meval\u001b[0m(code_obj, \u001b[96mself\u001b[0m.user_global_ns, \u001b[96mself\u001b[0m.user_ns)               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3552 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3553 \u001b[2m│   │   │   │   │   \u001b[0mexec(code_obj, \u001b[96mself\u001b[0m.user_global_ns, \u001b[96mself\u001b[0m.user_ns)                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3554 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfinally\u001b[0m:                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3555 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Reset our crash handler in place\u001b[0m                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m3556 \u001b[0m\u001b[2m│   │   │   │   \u001b[0msys.excepthook = old_excepthook                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mModuleNotFoundError: \u001b[0mNo module named \u001b[32m'pynvml'\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["import pynvml\n","from pynvml import *  # provides access to NVIDIA's NVML (NVIDIA Management Library).enables  querying and monitoring of NVIDIA GPU devices.\n","\n","\n","def print_gpu_utilization():\n","    nvmlInit()  # Initializing the NVML library\n","    handle = nvmlDeviceGetHandleByIndex(0)  # Obtaining a handle for the GPU device at index 0\n","    info = nvmlDeviceGetMemoryInfo(handle)  # Retrieving memory information for the GPU device\n","    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")  # Printing the GPU memory occupied in megabytes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZIt0kL3HmS2","outputId":"aca8669a-9ff0-48f4-cecc-edb28ec5d535"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU memory occupied: 440 MB.\n"]}],"source":["print_gpu_utilization()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275,"referenced_widgets":["81d97307ba0f44588926bd6ab2c1f1f8","8e169732a22c482891caec12ff249684","916533a81c714eff9174381ac399df45","72f967ba3cf74925889a6ba9c0e390da","0680280169104812a64d56c97c6f5c31","4ea2c126d3aa49168a17d5a66b47654a","f0d71bdf3e8645f4b79ca1dadaeacb00","52d6a7290c044c9e83a46714b01dee29","9423f84a5cf242858749cc2c6a4eac4b","57522fafee1a40a5b49a6d21b9bba9e5","fcd740da83e2474c80c980b76d16537d","fb45144ae2644e19b2b550ce77235b28","bb54ccd5ed9b4778aa997b500c875f4a","053803395e71499ca303704201ece745","588dd15a80b84dfbb7bed9771eaa2286","176d43dc0ede4449bfd30ab983bc6fef","29a4862e906240b6b04b2825bb308f99","484e9c66afb742a6986e52ab6cf30be8","d3ccd6d9110244a8a8dee2a5254a324f","040c25449aa7480998acc0607892600b","11809a20987549328375e3b17f5a93ce","6b602be825474f34b90fa8743d6e5e25","908c7e1136554862b7c0ea3da5b9813a","92970c91b8074832904274ce64aa77d4","cd72480d5a5a4396b7bd081dff1da872","7872b95248484b999b9e78f7a88f2877","6e1f9c0bc8664f74b2dd01d8e7c898ac","758c958a230445a7ad128b9c08bd02db","caee3ee235554dc1a05bec9cc9178cd0","62875bff85ac4a52b559bf7b5b554dca","9376687314994f87a704a88e62b66310","697d1a090a2c4e2abff7d9046f306972","32a7ed3b2ee647b99deef944ec94a37a","57a2baeeb8324908bba19d22ee80cf94","e14dae13c763464ca661f57b9828813d","7913f9c5088c421594bc9cffd8b57006","afae564e13a741d39ac2520c3bbddc43","1005b82dbe5c475cac45423afb941cd8","62ddc93d607f4bd8a82d5a0619731206","b3e945fe2fbd471bb0726821bd8d7816","84b0f182038d43d9af1f55424b479319","5bf9ef167f5243438eae2f96add18878","1eca1db3572c4b84ab2e29428cb01d8e","f955c82fada347ecb8236edf1b07bb5b","faa8e34ce80f4ec79b2f3cf45a71406b","b8d923ee3c644b0bba4597b5721fcfa5","7a277c0227814814bd7cf03eb839195a","8b8ff22e75b84cf8991f1f5080e94f49","a00c6b6753364020bb40a2090e5cef24","785b21a9e6fe4f4990f7c23a15459d5a","ad5b5d9a71a64307b8102522b001f732","85e64bb50a91496db0fe0b0d767fbfaa","446bd2396f6c4f11a3750b300e992b6a","b4cb772627e04025aa049fdb7965af59","a4fa3061c8e54c2ba4ed00f002f9d788","dcbb902c5cf04fc9a8fed3b55a2ed261","cceed486db064b4d94ab71f084f0c80f","f819f8bf627545a2a77e3a9a0f232965","a28029105ed14f15a9242ecf63dc176c","045f9378f8464aa7afa7f5168849590a","29fed1432ef14704bbafbfbaf8820f79","db906812858c4abab1eabba86e0b0a38","e3b3b3abe3784c6ab7296612e6d28c57","e834da0c15fb4206b85d21b08caa77a2","531ce833835e4bcba4ef3c295d708a2f","5ab3c58751994c3d9180e6746f7405d2"]},"executionInfo":{"elapsed":15845,"status":"ok","timestamp":1688918332927,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":420},"id":"WoOOaisR1Htv","outputId":"884880a9-0608-417e-c74c-78b829c08e02"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1c2c54ff78a78a92/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81d97307ba0f44588926bd6ab2c1f1f8","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb45144ae2644e19b2b550ce77235b28","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"908c7e1136554862b7c0ea3da5b9813a","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57a2baeeb8324908bba19d22ee80cf94","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"faa8e34ce80f4ec79b2f3cf45a71406b","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1c2c54ff78a78a92/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcbb902c5cf04fc9a8fed3b55a2ed261","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["train[1]: kee .\n","train[1] post shuffle: And then Cannata and Doss order a hit... 'cause she snuffed out their old friend.\n","val[1]: i yeah it's more yeah .\n","val[1] post shuffle: Do you think he's crying, weeping violently in the depths of the sea?\n","test[1] post shuffle: kangaroo .\n","test[1] post shuffle: Yes, but I think they're not unrelated.\n"]}],"source":["#load datasets\n","ds_dict=create_ds_dict(\"\")\n","train=ds_dict[\"train\"][\"sentence\"]\n","print(\"train[1]:\",train[1])\n","random.shuffle(train)\n","print(\"train[1] post shuffle:\", train[1])\n","val=ds_dict[\"validation\"][\"sentence\"]\n","print(\"val[1]:\", val[1])\n","random.shuffle(val)\n","print(\"val[1] post shuffle:\", val[1])\n","test=ds_dict[\"test\"][\"sentence\"]\n","print(\"test[1] post shuffle:\", test[1])\n","random.shuffle(test)\n","print(\"test[1] post shuffle:\", test[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["78ef294f552440e2ad7e19d87621a1a1","810c0b4edd8942eb9e697a6902d24523","8096eb9928724b33adadf2ef676195b3","7d7d980706634a38b8aad20fbcf66cd0","03287a9a8626409d8bf10c0fa0913672","742abddf340b43a49aabd9963ea9e5e7","978e5f11a53643e88a450c597d632792","93ac13ee58b9432d8ff5276ef31bde88","ea64058ea2cd401b8c902e44cc7b4315","c32a7500bddf48a8a66ac242c5306d58","32345273165c445aa4ea3135ab9fb16d"]},"executionInfo":{"elapsed":191009,"status":"ok","timestamp":1688918529230,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":420},"id":"_CpWjCaWHmS2","outputId":"10d1c3ac-56f4-4ada-873b-d3d40c5d787f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78ef294f552440e2ad7e19d87621a1a1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/918982 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["batches created: 28937\n","OPTConfig {\n","  \"_remove_final_layer_norm\": false,\n","  \"activation_function\": \"relu\",\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 2,\n","  \"do_layer_norm_before\": true,\n","  \"dropout\": 0.1,\n","  \"enable_bias\": true,\n","  \"eos_token_id\": 2,\n","  \"ffn_dim\": 3072,\n","  \"hidden_size\": 768,\n","  \"init_std\": 0.02,\n","  \"layer_norm_elementwise_affine\": true,\n","  \"layerdrop\": 0.0,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"opt\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.30.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50272,\n","  \"word_embed_proj_dim\": 768\n","}\n","\n","OPTConfig {\n","  \"_remove_final_layer_norm\": false,\n","  \"activation_function\": \"relu\",\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 2,\n","  \"do_layer_norm_before\": true,\n","  \"dropout\": 0.1,\n","  \"enable_bias\": true,\n","  \"eos_token_id\": 2,\n","  \"ffn_dim\": 3072,\n","  \"hidden_size\": 768,\n","  \"init_std\": 0.02,\n","  \"layer_norm_elementwise_affine\": true,\n","  \"layerdrop\": 0.0,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"opt\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.30.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50272,\n","  \"word_embed_proj_dim\": 768\n","}\n","\n","OPTForCausalLM(\n","  (model): OPTModel(\n","    (decoder): OPTDecoder(\n","      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n","      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n","      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x OPTDecoderLayer(\n","          (self_attn): OPTAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): ReLU()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",")\n","OPT size: 125.2M parameters\n"]},{"data":{"text/plain":["OPTForCausalLM(\n","  (model): OPTModel(\n","    (decoder): OPTDecoder(\n","      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n","      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n","      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-11): 12 x OPTDecoderLayer(\n","          (self_attn): OPTAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): ReLU()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer=create_BPE_tok(train,tok_dir)\n","trainDL=create_tokenized_batches(train,tokenizer,context_length,batch_size,\"trainDL.csv\")\n","batches_per_epoch=len(trainDL[:100])\n","total_steps=num_train_epochs*batches_per_epoch #update gradient every epoch\n","model=init_OPT_CLM() #OPT125M with causal LM head for training\n","model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300,"referenced_widgets":["ea94cb675e3d4813b772d6ef961d187a","e51c468902bd4494ba8f3807178e81ff","4ae8fdb0d7a448d2a229d719fc83e1bf","a5ac2aa93f444675adfd62bc0d3a3303","dbe9066934514571b161dead31e18cd4","1844d7cae7f24e63980d63915b327cfe","2daca1cdae314ac2a11e60b8177c4ff0","f889a097fd9b4eeda463f2a303efa6d6","2dad4b64a07144d7ab605f43721a1db3","99ca170653224f7c980e80804b6d270d","0d2eaaa4c510412ea86626f6708e39cf"]},"executionInfo":{"elapsed":123582,"status":"ok","timestamp":1688918653600,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":420},"id":"81zBhu0T1RxO","outputId":"1a86f81c-c61c-429d-8117-c6f6a510a557"},"outputs":[{"name":"stdout","output_type":"stream","text":["AdamW (\n","Parameter Group 0\n","    betas: (0.9, 0.95)\n","    correct_bias: True\n","    eps: 1e-06\n","    initial_lr: 5e-05\n","    lr: 0.0\n","    weight_decay: 0.1\n",")\n","<torch.optim.lr_scheduler.LambdaLR object at 0x7efd5e1de080>\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea94cb675e3d4813b772d6ef961d187a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/883051 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["batches created: 27806\n"]}],"source":["es=EarlyStopping(patience=3)\n","optimizer,lr_scheduler=create_optimizer_and_lr_scheduler(total_steps)\n","print(optimizer)\n","print(lr_scheduler)\n","valDL=create_tokenized_batches(val,tokenizer,context_length,batch_size,\"valDL.csv\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276,"referenced_widgets":["1124cf75c50e41d2980b54af5692ee62","3d6570a16d3f48fd84f9176ca106d028","4943f3bc729341959bb531381d6bf5b0","b94c490755684955948fe3e8928b2b9b","466b72f45c99499cbcb75565c3d688db","e4002cc2f9044fd1b389172827252416","1527ac2d945140959091b1adda0b92d1","949dc34ab0774b07b57a4c1dd09ee3b5","abf5844001134e67864943cd834dc97f","c29db15525f54dc6b77e4af625a7d222","9bebb801b77a4db5b0b0e19a832f9551","97634bce08c74d3c9287fe9aafcba23f","3fd9182d29f44e8da3310211595c3bba","902a344e0a004fb49f3147e5a71b96d4","d46d9581a9ad42c6856efc4b6f803190","62d9bf79b33e4cf08b382f415543e1ac","5cac71ca3d6b44d2bd156cf68e30da1e","2ed054418d86424eac8f67b0bf15763c","d611b7d319e247dfa4b73376150be49c","650269061d4e445b9518f299203afd4f","e24da643e09d4c7a94e954eaa4305af4","b7a76c50b0fb474fa9899e4c21c66e28","d9ab39bfd92749ff87119058910789e6","87d9ac1c30ff472b98d52b6131991e53","cacd04097e224476a40dda8cc84d424e","934c19e945ed4e209853c4143bba06fe","888f287ac1cd424a9bcbeb4ed3eaa01d","a979c6fb869b4d208f871350519388c7","f4e9203bb39040d38881f720003a6cd3","727c1488f81445c8b804d350ab7d7d99","832b39302ffc4fb3a037e123003c394e","8066847e02f74e1a98050c4ecf3d2b7b","0e3d7c01126d49c5a87c20d2a2499ceb","649907eb4b984362bb9829dfe139e862","91a215f5f9004fdfba663be38aa74855","0032f450826547cbb0b36b55393c3619","7510dc98074540e1b38ad6812d1d042d","6ebe7a3a606743dfaa2d8a4233fd2035","362640f77d7144eebd8f962defa30f6f","ab39ce3ec7a64e81ac0fe98c12b1dbd6","6d4d4a239ed0479fa8d90c3b906d78ca","b847a30c0b82427abe1b1a1b7b3ff722","2525b639902848c2810a965c402afecd","16abb3796b074176b4c2e976f0839c0b"]},"executionInfo":{"elapsed":2568703,"status":"error","timestamp":1688925860463,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":420},"id":"8HcrtecJ1WFB","outputId":"4917d151-da27-48bb-d322-498da12f5054"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1124cf75c50e41d2980b54af5692ee62","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97634bce08c74d3c9287fe9aafcba23f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/27806 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9ab39bfd92749ff87119058910789e6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"649907eb4b984362bb9829dfe139e862","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/27806 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_loop</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_one_epoch</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">19</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">evaluate</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;dictcomp&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n","</pre>\n"],"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92mtraining_loop\u001b[0m:\u001b[94m4\u001b[0m                                                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92mtrain_one_epoch\u001b[0m:\u001b[94m19\u001b[0m                                                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92mevaluate\u001b[0m:\u001b[94m7\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<dictcomp>\u001b[0m:\u001b[94m7\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mKeyboardInterrupt\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["tval=training_loop(model,lr_scheduler,optimizer,device,num_epochs=5,train_dataloader=trainDL[:100],eval_dataloader=valDL,total_training_steps=total_steps,completed_steps=0,log_steps=1000,t_log=\"testing_train_log.csv\",eval_steps=batches_per_epoch,e_log=\"testing_eval_log.csv\",early_stopper=es,output_dir=(sync_dir+\"OPT125MV1\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["34fdb39ef6624c84b259fc7388299f93","b13460b5e4854d7bb69c246300a5ddfe","f04b62a7f17445eaa15bf1324f09c922","5f84210c273e437e9fbb464fb8524d2f","e0a453d415c64f97a0050b1d9b4dd302","f4ee2f289c394796bfef30fa159c5aac","b82f1113025e425384724d391bfcd09a","34af808a7afe407c88c19b02b036e7ee","1296effba1754d94a847ae65417dde14","a600c2a9ff1d4e19bfd143494fbf1a9d","d68f9e6cace9418bb4210119da85202a","f12c6dfce67945649ba6fb736b574121","c2d0066b73e8495387563b220a04a9ab","fd342fb0a7da4b04b7a16d792c21bdb5","744a6a30ee2243cfa538db6cf5006b17","d4713c8427e5458ea1d94fd442464f0c","ce98b4f0ec394029b55fcd32798ee3fd","0ff9568eed1d4131b7527aeba12676ff","c79cb7e7bbfb49a3b4d78f7a23eb2444","bf1a4821cc664246aa0402acea1d743f","2ff76b25ea544e148073d13f5363e407","2de8e46c56564283a490999ff56b67f1"]},"id":"MjhMyyU61Zlg","outputId":"880e6c45-1977-4497-ba79-0458ecc8dac4"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34fdb39ef6624c84b259fc7388299f93","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/909795 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["batches created: 28668\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f12c6dfce67945649ba6fb736b574121","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/28668 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tval=0\n","print(tval)\n","testDL=create_tokenized_batches(test,tokenizer,context_length,batch_size,\"testDL.csv\")\n","tloss,tppl=evaluate(model,testDL,device)\n","resultDict={\"T-value\":tval,\"training loss\":tloss,\"training perplexity\":tppl}\n","print(resultDict)\n","with open(\"results.csv\", 'w', newline='') as file: #may throw error , while coppying and queing run of cell forgot to change fields to resultDict.keys() and writerows(resultDict) to writerow(resultDict.values())\n","  writer = csv.DictWriter(file, fieldnames=resultDict.keys())\n","  writer.writeheader()\n","  writer.writerow(resultDict.values())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1688898890953,"user":{"displayName":"Aryaman Chobey '25","userId":"05384327857293136953"},"user_tz":420},"id":"czOMgXSjNvmJ","outputId":"865bc74f-937b-4d06-b570-570abb2e0f85"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bkdnL89oOvw"},"outputs":[],"source":["torch.cuda.empty_cache()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0032f450826547cbb0b36b55393c3619":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d4d4a239ed0479fa8d90c3b906d78ca","max":27806,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b847a30c0b82427abe1b1a1b7b3ff722","value":7842}},"03287a9a8626409d8bf10c0fa0913672":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"040c25449aa7480998acc0607892600b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"045f9378f8464aa7afa7f5168849590a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"053803395e71499ca303704201ece745":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3ccd6d9110244a8a8dee2a5254a324f","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_040c25449aa7480998acc0607892600b","value":3}},"0680280169104812a64d56c97c6f5c31":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d2eaaa4c510412ea86626f6708e39cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e3d7c01126d49c5a87c20d2a2499ceb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ff9568eed1d4131b7527aeba12676ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1005b82dbe5c475cac45423afb941cd8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"1124cf75c50e41d2980b54af5692ee62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d6570a16d3f48fd84f9176ca106d028","IPY_MODEL_4943f3bc729341959bb531381d6bf5b0","IPY_MODEL_b94c490755684955948fe3e8928b2b9b"],"layout":"IPY_MODEL_466b72f45c99499cbcb75565c3d688db"}},"11809a20987549328375e3b17f5a93ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1296effba1754d94a847ae65417dde14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1527ac2d945140959091b1adda0b92d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16abb3796b074176b4c2e976f0839c0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"176d43dc0ede4449bfd30ab983bc6fef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1844d7cae7f24e63980d63915b327cfe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eca1db3572c4b84ab2e29428cb01d8e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2525b639902848c2810a965c402afecd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29a4862e906240b6b04b2825bb308f99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29fed1432ef14704bbafbfbaf8820f79":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2daca1cdae314ac2a11e60b8177c4ff0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dad4b64a07144d7ab605f43721a1db3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2de8e46c56564283a490999ff56b67f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ed054418d86424eac8f67b0bf15763c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ff76b25ea544e148073d13f5363e407":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32345273165c445aa4ea3135ab9fb16d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32a7ed3b2ee647b99deef944ec94a37a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34af808a7afe407c88c19b02b036e7ee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34fdb39ef6624c84b259fc7388299f93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b13460b5e4854d7bb69c246300a5ddfe","IPY_MODEL_f04b62a7f17445eaa15bf1324f09c922","IPY_MODEL_5f84210c273e437e9fbb464fb8524d2f"],"layout":"IPY_MODEL_e0a453d415c64f97a0050b1d9b4dd302"}},"362640f77d7144eebd8f962defa30f6f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d6570a16d3f48fd84f9176ca106d028":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4002cc2f9044fd1b389172827252416","placeholder":"​","style":"IPY_MODEL_1527ac2d945140959091b1adda0b92d1","value":" 50%"}},"3fd9182d29f44e8da3310211595c3bba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cac71ca3d6b44d2bd156cf68e30da1e","placeholder":"​","style":"IPY_MODEL_2ed054418d86424eac8f67b0bf15763c","value":"100%"}},"446bd2396f6c4f11a3750b300e992b6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"466b72f45c99499cbcb75565c3d688db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"484e9c66afb742a6986e52ab6cf30be8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4943f3bc729341959bb531381d6bf5b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_949dc34ab0774b07b57a4c1dd09ee3b5","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abf5844001134e67864943cd834dc97f","value":100}},"4ae8fdb0d7a448d2a229d719fc83e1bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f889a097fd9b4eeda463f2a303efa6d6","max":883051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2dad4b64a07144d7ab605f43721a1db3","value":883051}},"4ea2c126d3aa49168a17d5a66b47654a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52d6a7290c044c9e83a46714b01dee29":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"531ce833835e4bcba4ef3c295d708a2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57522fafee1a40a5b49a6d21b9bba9e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57a2baeeb8324908bba19d22ee80cf94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e14dae13c763464ca661f57b9828813d","IPY_MODEL_7913f9c5088c421594bc9cffd8b57006","IPY_MODEL_afae564e13a741d39ac2520c3bbddc43"],"layout":"IPY_MODEL_1005b82dbe5c475cac45423afb941cd8"}},"588dd15a80b84dfbb7bed9771eaa2286":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11809a20987549328375e3b17f5a93ce","placeholder":"​","style":"IPY_MODEL_6b602be825474f34b90fa8743d6e5e25","value":" 3/3 [00:00&lt;00:00, 80.71it/s]"}},"5ab3c58751994c3d9180e6746f7405d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bf9ef167f5243438eae2f96add18878":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5cac71ca3d6b44d2bd156cf68e30da1e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f84210c273e437e9fbb464fb8524d2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a600c2a9ff1d4e19bfd143494fbf1a9d","placeholder":"​","style":"IPY_MODEL_d68f9e6cace9418bb4210119da85202a","value":" 909795/909795 [02:11&lt;00:00, 8085.98it/s]"}},"62875bff85ac4a52b559bf7b5b554dca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"62d9bf79b33e4cf08b382f415543e1ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62ddc93d607f4bd8a82d5a0619731206":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"649907eb4b984362bb9829dfe139e862":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91a215f5f9004fdfba663be38aa74855","IPY_MODEL_0032f450826547cbb0b36b55393c3619","IPY_MODEL_7510dc98074540e1b38ad6812d1d042d"],"layout":"IPY_MODEL_6ebe7a3a606743dfaa2d8a4233fd2035"}},"650269061d4e445b9518f299203afd4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"697d1a090a2c4e2abff7d9046f306972":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b602be825474f34b90fa8743d6e5e25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d4d4a239ed0479fa8d90c3b906d78ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e1f9c0bc8664f74b2dd01d8e7c898ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"6ebe7a3a606743dfaa2d8a4233fd2035":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"727c1488f81445c8b804d350ab7d7d99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72f967ba3cf74925889a6ba9c0e390da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57522fafee1a40a5b49a6d21b9bba9e5","placeholder":"​","style":"IPY_MODEL_fcd740da83e2474c80c980b76d16537d","value":" 3/3 [00:00&lt;00:00, 60.59it/s]"}},"742abddf340b43a49aabd9963ea9e5e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"744a6a30ee2243cfa538db6cf5006b17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ff76b25ea544e148073d13f5363e407","placeholder":"​","style":"IPY_MODEL_2de8e46c56564283a490999ff56b67f1","value":" 76/28668 [00:15&lt;1:32:27,  5.15it/s]"}},"7510dc98074540e1b38ad6812d1d042d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2525b639902848c2810a965c402afecd","placeholder":"​","style":"IPY_MODEL_16abb3796b074176b4c2e976f0839c0b","value":" 7842/27806 [26:01&lt;1:03:33,  5.23it/s]"}},"758c958a230445a7ad128b9c08bd02db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"785b21a9e6fe4f4990f7c23a15459d5a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7872b95248484b999b9e78f7a88f2877":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_697d1a090a2c4e2abff7d9046f306972","placeholder":"​","style":"IPY_MODEL_32a7ed3b2ee647b99deef944ec94a37a","value":" 910000/0 [00:04&lt;00:00, 171041.54 examples/s]"}},"78ef294f552440e2ad7e19d87621a1a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_810c0b4edd8942eb9e697a6902d24523","IPY_MODEL_8096eb9928724b33adadf2ef676195b3","IPY_MODEL_7d7d980706634a38b8aad20fbcf66cd0"],"layout":"IPY_MODEL_03287a9a8626409d8bf10c0fa0913672"}},"7913f9c5088c421594bc9cffd8b57006":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_84b0f182038d43d9af1f55424b479319","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5bf9ef167f5243438eae2f96add18878","value":1}},"7a277c0227814814bd7cf03eb839195a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_85e64bb50a91496db0fe0b0d767fbfaa","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_446bd2396f6c4f11a3750b300e992b6a","value":1}},"7d7d980706634a38b8aad20fbcf66cd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c32a7500bddf48a8a66ac242c5306d58","placeholder":"​","style":"IPY_MODEL_32345273165c445aa4ea3135ab9fb16d","value":" 918982/918982 [02:21&lt;00:00, 5495.39it/s]"}},"8066847e02f74e1a98050c4ecf3d2b7b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8096eb9928724b33adadf2ef676195b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93ac13ee58b9432d8ff5276ef31bde88","max":918982,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea64058ea2cd401b8c902e44cc7b4315","value":918982}},"810c0b4edd8942eb9e697a6902d24523":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_742abddf340b43a49aabd9963ea9e5e7","placeholder":"​","style":"IPY_MODEL_978e5f11a53643e88a450c597d632792","value":"100%"}},"81d97307ba0f44588926bd6ab2c1f1f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e169732a22c482891caec12ff249684","IPY_MODEL_916533a81c714eff9174381ac399df45","IPY_MODEL_72f967ba3cf74925889a6ba9c0e390da"],"layout":"IPY_MODEL_0680280169104812a64d56c97c6f5c31"}},"832b39302ffc4fb3a037e123003c394e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84b0f182038d43d9af1f55424b479319":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"85e64bb50a91496db0fe0b0d767fbfaa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"87d9ac1c30ff472b98d52b6131991e53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a979c6fb869b4d208f871350519388c7","placeholder":"​","style":"IPY_MODEL_f4e9203bb39040d38881f720003a6cd3","value":" 50%"}},"888f287ac1cd424a9bcbeb4ed3eaa01d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b8ff22e75b84cf8991f1f5080e94f49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4cb772627e04025aa049fdb7965af59","placeholder":"​","style":"IPY_MODEL_a4fa3061c8e54c2ba4ed00f002f9d788","value":" 880000/0 [00:02&lt;00:00, 312106.53 examples/s]"}},"8e169732a22c482891caec12ff249684":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ea2c126d3aa49168a17d5a66b47654a","placeholder":"​","style":"IPY_MODEL_f0d71bdf3e8645f4b79ca1dadaeacb00","value":"Downloading data files: 100%"}},"902a344e0a004fb49f3147e5a71b96d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d611b7d319e247dfa4b73376150be49c","max":27806,"min":0,"orientation":"horizontal","style":"IPY_MODEL_650269061d4e445b9518f299203afd4f","value":27806}},"908c7e1136554862b7c0ea3da5b9813a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92970c91b8074832904274ce64aa77d4","IPY_MODEL_cd72480d5a5a4396b7bd081dff1da872","IPY_MODEL_7872b95248484b999b9e78f7a88f2877"],"layout":"IPY_MODEL_6e1f9c0bc8664f74b2dd01d8e7c898ac"}},"916533a81c714eff9174381ac399df45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52d6a7290c044c9e83a46714b01dee29","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9423f84a5cf242858749cc2c6a4eac4b","value":3}},"91a215f5f9004fdfba663be38aa74855":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_362640f77d7144eebd8f962defa30f6f","placeholder":"​","style":"IPY_MODEL_ab39ce3ec7a64e81ac0fe98c12b1dbd6","value":" 28%"}},"92970c91b8074832904274ce64aa77d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_758c958a230445a7ad128b9c08bd02db","placeholder":"​","style":"IPY_MODEL_caee3ee235554dc1a05bec9cc9178cd0","value":"Generating train split: "}},"934c19e945ed4e209853c4143bba06fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8066847e02f74e1a98050c4ecf3d2b7b","placeholder":"​","style":"IPY_MODEL_0e3d7c01126d49c5a87c20d2a2499ceb","value":" 99/200 [26:59&lt;01:18,  1.28it/s]"}},"9376687314994f87a704a88e62b66310":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93ac13ee58b9432d8ff5276ef31bde88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9423f84a5cf242858749cc2c6a4eac4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"949dc34ab0774b07b57a4c1dd09ee3b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97634bce08c74d3c9287fe9aafcba23f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3fd9182d29f44e8da3310211595c3bba","IPY_MODEL_902a344e0a004fb49f3147e5a71b96d4","IPY_MODEL_d46d9581a9ad42c6856efc4b6f803190"],"layout":"IPY_MODEL_62d9bf79b33e4cf08b382f415543e1ac"}},"978e5f11a53643e88a450c597d632792":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99ca170653224f7c980e80804b6d270d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bebb801b77a4db5b0b0e19a832f9551":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a00c6b6753364020bb40a2090e5cef24":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"a28029105ed14f15a9242ecf63dc176c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_531ce833835e4bcba4ef3c295d708a2f","placeholder":"​","style":"IPY_MODEL_5ab3c58751994c3d9180e6746f7405d2","value":" 3/3 [00:00&lt;00:00, 37.65it/s]"}},"a4fa3061c8e54c2ba4ed00f002f9d788":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5ac2aa93f444675adfd62bc0d3a3303":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99ca170653224f7c980e80804b6d270d","placeholder":"​","style":"IPY_MODEL_0d2eaaa4c510412ea86626f6708e39cf","value":" 883051/883051 [02:03&lt;00:00, 7969.05it/s]"}},"a600c2a9ff1d4e19bfd143494fbf1a9d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a979c6fb869b4d208f871350519388c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab39ce3ec7a64e81ac0fe98c12b1dbd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abf5844001134e67864943cd834dc97f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad5b5d9a71a64307b8102522b001f732":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afae564e13a741d39ac2520c3bbddc43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1eca1db3572c4b84ab2e29428cb01d8e","placeholder":"​","style":"IPY_MODEL_f955c82fada347ecb8236edf1b07bb5b","value":" 880000/0 [00:03&lt;00:00, 251428.32 examples/s]"}},"b13460b5e4854d7bb69c246300a5ddfe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4ee2f289c394796bfef30fa159c5aac","placeholder":"​","style":"IPY_MODEL_b82f1113025e425384724d391bfcd09a","value":"100%"}},"b3e945fe2fbd471bb0726821bd8d7816":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4cb772627e04025aa049fdb7965af59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7a76c50b0fb474fa9899e4c21c66e28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b82f1113025e425384724d391bfcd09a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b847a30c0b82427abe1b1a1b7b3ff722":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8d923ee3c644b0bba4597b5721fcfa5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_785b21a9e6fe4f4990f7c23a15459d5a","placeholder":"​","style":"IPY_MODEL_ad5b5d9a71a64307b8102522b001f732","value":"Generating test split: "}},"b94c490755684955948fe3e8928b2b9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c29db15525f54dc6b77e4af625a7d222","placeholder":"​","style":"IPY_MODEL_9bebb801b77a4db5b0b0e19a832f9551","value":" 100/200 [1:33:07&lt;46:05:26, 1659.27s/it]"}},"bb54ccd5ed9b4778aa997b500c875f4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29a4862e906240b6b04b2825bb308f99","placeholder":"​","style":"IPY_MODEL_484e9c66afb742a6986e52ab6cf30be8","value":"Extracting data files: 100%"}},"bf1a4821cc664246aa0402acea1d743f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c29db15525f54dc6b77e4af625a7d222":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2d0066b73e8495387563b220a04a9ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce98b4f0ec394029b55fcd32798ee3fd","placeholder":"​","style":"IPY_MODEL_0ff9568eed1d4131b7527aeba12676ff","value":"  0%"}},"c32a7500bddf48a8a66ac242c5306d58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c79cb7e7bbfb49a3b4d78f7a23eb2444":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cacd04097e224476a40dda8cc84d424e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_727c1488f81445c8b804d350ab7d7d99","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_832b39302ffc4fb3a037e123003c394e","value":99}},"caee3ee235554dc1a05bec9cc9178cd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cceed486db064b4d94ab71f084f0c80f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29fed1432ef14704bbafbfbaf8820f79","placeholder":"​","style":"IPY_MODEL_db906812858c4abab1eabba86e0b0a38","value":"100%"}},"cd72480d5a5a4396b7bd081dff1da872":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_62875bff85ac4a52b559bf7b5b554dca","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9376687314994f87a704a88e62b66310","value":1}},"ce98b4f0ec394029b55fcd32798ee3fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3ccd6d9110244a8a8dee2a5254a324f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d46d9581a9ad42c6856efc4b6f803190":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e24da643e09d4c7a94e954eaa4305af4","placeholder":"​","style":"IPY_MODEL_b7a76c50b0fb474fa9899e4c21c66e28","value":" 27806/27806 [1:31:59&lt;00:00,  6.43it/s]"}},"d4713c8427e5458ea1d94fd442464f0c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d611b7d319e247dfa4b73376150be49c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d68f9e6cace9418bb4210119da85202a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9ab39bfd92749ff87119058910789e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_87d9ac1c30ff472b98d52b6131991e53","IPY_MODEL_cacd04097e224476a40dda8cc84d424e","IPY_MODEL_934c19e945ed4e209853c4143bba06fe"],"layout":"IPY_MODEL_888f287ac1cd424a9bcbeb4ed3eaa01d"}},"db906812858c4abab1eabba86e0b0a38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbe9066934514571b161dead31e18cd4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcbb902c5cf04fc9a8fed3b55a2ed261":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cceed486db064b4d94ab71f084f0c80f","IPY_MODEL_f819f8bf627545a2a77e3a9a0f232965","IPY_MODEL_a28029105ed14f15a9242ecf63dc176c"],"layout":"IPY_MODEL_045f9378f8464aa7afa7f5168849590a"}},"e0a453d415c64f97a0050b1d9b4dd302":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e14dae13c763464ca661f57b9828813d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62ddc93d607f4bd8a82d5a0619731206","placeholder":"​","style":"IPY_MODEL_b3e945fe2fbd471bb0726821bd8d7816","value":"Generating validation split: "}},"e24da643e09d4c7a94e954eaa4305af4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3b3b3abe3784c6ab7296612e6d28c57":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4002cc2f9044fd1b389172827252416":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e51c468902bd4494ba8f3807178e81ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1844d7cae7f24e63980d63915b327cfe","placeholder":"​","style":"IPY_MODEL_2daca1cdae314ac2a11e60b8177c4ff0","value":"100%"}},"e834da0c15fb4206b85d21b08caa77a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea64058ea2cd401b8c902e44cc7b4315":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea94cb675e3d4813b772d6ef961d187a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e51c468902bd4494ba8f3807178e81ff","IPY_MODEL_4ae8fdb0d7a448d2a229d719fc83e1bf","IPY_MODEL_a5ac2aa93f444675adfd62bc0d3a3303"],"layout":"IPY_MODEL_dbe9066934514571b161dead31e18cd4"}},"f04b62a7f17445eaa15bf1324f09c922":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_34af808a7afe407c88c19b02b036e7ee","max":909795,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1296effba1754d94a847ae65417dde14","value":909795}},"f0d71bdf3e8645f4b79ca1dadaeacb00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f12c6dfce67945649ba6fb736b574121":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2d0066b73e8495387563b220a04a9ab","IPY_MODEL_fd342fb0a7da4b04b7a16d792c21bdb5","IPY_MODEL_744a6a30ee2243cfa538db6cf5006b17"],"layout":"IPY_MODEL_d4713c8427e5458ea1d94fd442464f0c"}},"f4e9203bb39040d38881f720003a6cd3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4ee2f289c394796bfef30fa159c5aac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f819f8bf627545a2a77e3a9a0f232965":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3b3b3abe3784c6ab7296612e6d28c57","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e834da0c15fb4206b85d21b08caa77a2","value":3}},"f889a097fd9b4eeda463f2a303efa6d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f955c82fada347ecb8236edf1b07bb5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"faa8e34ce80f4ec79b2f3cf45a71406b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8d923ee3c644b0bba4597b5721fcfa5","IPY_MODEL_7a277c0227814814bd7cf03eb839195a","IPY_MODEL_8b8ff22e75b84cf8991f1f5080e94f49"],"layout":"IPY_MODEL_a00c6b6753364020bb40a2090e5cef24"}},"fb45144ae2644e19b2b550ce77235b28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bb54ccd5ed9b4778aa997b500c875f4a","IPY_MODEL_053803395e71499ca303704201ece745","IPY_MODEL_588dd15a80b84dfbb7bed9771eaa2286"],"layout":"IPY_MODEL_176d43dc0ede4449bfd30ab983bc6fef"}},"fcd740da83e2474c80c980b76d16537d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd342fb0a7da4b04b7a16d792c21bdb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c79cb7e7bbfb49a3b4d78f7a23eb2444","max":28668,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf1a4821cc664246aa0402acea1d743f","value":76}}}}},"nbformat":4,"nbformat_minor":0}